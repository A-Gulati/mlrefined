<!DOCTYPE html>

<html>
	
	<head>
		<!-- always use https, not http --> 
		<script type="text/javascript" src='https://ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js'></script>
		<link rel="stylesheet" href="html/CSS/home.css">
		
		<!-- need this for navigation menu --> 
		<link rel="stylesheet" href="html/CSS/navbar.css">
		
		<style>
			.back-text-color { /* color of text on the back of each card */
				color: #333;
			}
			
		 </style>	
		 
		<!-- subscription -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
		<link rel="stylesheet" href="html/CSS/subscription.css">
		 
	</head>
		 

	<body>
	
		<!-- Navigation menu -->	
		<br/><br/><br/>
		<div>
    		<ul>
        		<li class="menu-item active"><a href="index.html"><span class="menu-item">HOME</span></a></li>
        	<!--	<li class="menu-item"><a href="html/pages/presentations.html"><span class="menu-item">PRESENTATIONS</span></a></li>
        		<li class="menu-item"><a href="html/pages/gallery.html"><span class="menu-item">GALLERY</span></a></li> -->
        		<li class="menu-item"><a href="html/pages/about.html"><span class="menu-item">ABOUT</span></a></li>
   			</ul>
		</div>
		
		
		<br/><br/>
		<p id="title">Machine Learning Refined</p>
		<p id="sub-title">
			
	

		This is a blog about machine learning / deep learning fundamentals built by the authors of the textbook <a class="redlink" target="_blank" href="http://mlrefined.com">Machine Learning Refined</a> published by Cambridge University Press.  

The posts - cut into short series - use careful writing and interactive coding widgets to provide an intuitive and playful way to learn about core concepts in AI- from some of the most basic to the most advanced.  
		
		<br>
		<br>

		Each and every post here is a Python Jupyter notebook - prettied up for the web - that you can download and run on your own machine by pulling <a class="redlink" target="_blank" href="https://github.com/jermwatt/mlrefined"> our repo</a>.  

		</p>
		<br/><br/>
		
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter01" checked />   
  			<label for="chapter01"><span>CHAPTER 1. Introduction to machine learning, deep learning, and reinforcement </span></label>
  			<div class="three_col">
  				<div class="description">
    				<p><strong>1.1. </strong> Machine learning, a framework for learning from data</p>
    				<p><strong>1.2. </strong> Introduction to problem types in machine learning coupled with overview of modern applications </p>
    				<p><strong>1.3. </strong> The basic building blocks of machine learning and how they all fit together</p>
  				</div>
  			</div>
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter02" checked/>
  			<label for="chapter02"><span>CHAPTER 2. Computational linear algebra and statistics</span></label>
  			<div class="four_col">
  				<div class="description">
    				<p><strong>2.1. </strong> Vectors and matrices, vector and matrix norms, linear functions and matrices </p>
    				<p><strong>2.2. </strong> Eigenvalues, eigenvectors and the power method </p>
    				<p><strong>2.3. </strong> Fundamentals of probability and statistics, discrete and continuous distributions </p>
    				<p><strong>2.4. </strong> Maximum likelihood approach and Bayes’ rule </p>
  				</div>
  			</div>
		</div>
	
	
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter03" checked/>   
  			<label for="chapter03"><span>CHAPTER 3. Computational calculus</span></label>
  			<div class="four_col">
  				<div class="description">
    				<p><strong>3.1. </strong> The derivative and computation graphs</p>
    				<p><strong>3.2. </strong> Automatic differentiation part 1: the forward method </p>
    				<p><strong>3.3. </strong> Taylor series approximation </p>
    				<p><strong>3.4. </strong> Vector valued derivatives </p>
  				</div>
  			</div>	
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter04" checked/>   
  			<label for="chapter04"><span>CHAPTER 4. Mathematical Optimization I: Gradient descent</span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>4.1. </strong> Unconstrained optimality conditions</p>
    				<p><strong>4.2. </strong> Naive and local search methods</p>
    				<p><strong>4.3. </strong> Gradient descent</p>
    				<p><strong>4.4. </strong> Step-length rules, backtracking line search</p>
  				</div>
			</div>
		</div>		

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter05" checked/>   
  			<label for="chapter05"><span>CHAPTER 5. Linear regression </span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>5.1. </strong> Linear regression: Least Squares and Least Absolute Deviations</p>
    				<p><strong>5.2. </strong> The probabilistic perspective on linear regression</p>
    				<p><strong>5.3. </strong> Performance metrics for regression</p>
    				<p><strong>5.4. </strong> Feature selection and regularization	</p>
  				</div>
			</div>
		</div>
		
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter06" checked/>   
  			<label for="chapter06"><span>CHAPTER 6. Linear two-class classification </span></label>
  			<div class="seven_col">
  				<div class="description">
    				<p><strong>6.1. </strong> From linear to logistic regression</p>
    				<p><strong>6.2. </strong> Logistic regression: geometric and probabilistic perspectives</p>
    				<p><strong>6.3. </strong> The classic perceptron and support vector machines</p>
    				<p><strong>6.4. </strong> A unified view of two-class classification</p>
    				<p><strong>6.5. </strong> Performance metrics for classification</p>
    				<p><strong>6.6. </strong> Principles of feature engineering </p>
    				<p><strong>6.7. </strong> General data pre-processing techniques</p>
  				</div>
			</div>
		</div>

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter07" checked/>   
  			<label for="chapter07"><span>CHAPTER 7. Linear multiclass classification</span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>7.1. </strong> One-versus-All classification</p>
    				<p><strong>7.2. </strong> Multiclass softmax classification: geometric and probabilistic perspectives</p>
    				<p><strong>7.3. </strong> A unified view of multiclass classification</p>
    				<p><strong>7.4. </strong> Performance metrics</p>
  				</div>
			</div>		
		</div>

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter08" checked/>   
  			<label for="chapter08"><span>CHAPTER 8. Mathematical Optimization II: Algorithms for dealing with large datasets</span></label>
  			<div class="five_col">	
  				<div class="description">
    				<p><strong>8.1. </strong> Feature scaling and the “long narrow valley” problem with gradient descent</p>
    				<p><strong>8.2. </strong> Scaling with data, stochastic gradient descent</p>
    				<p><strong>8.3. </strong> Newton’s method and Quasi-Newton methods</p>
    				<p><strong>8.4. </strong> Coordinate descent methods</p>
    				<p><strong>8.5. </strong> Projections and projection methods</p>
  				</div>
			</div>
		</div>	
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter09" checked/>   
  			<label for="chapter09"><span>CHAPTER 9. Linear dimension reduction techniques</span></label>
  			<div class="five_col">	
  				<div class="description">
    				<p><strong>9.1. </strong> Linear representations of data</p>
    				<p><strong>9.2. </strong> Principal component analysis (PCA): geometric and probabilistic perspectives</p>
    				<p><strong>9.3. </strong> K-means clustering</p>
    				<p><strong>9.4. </strong> Recommender systems</p>
    				<p><strong>9.5. </strong> The general matrix factorization framework</p>
  				</div>
			</div>		
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter10" checked/>   
  			<label for="chapter10"><span>CHAPTER 10. Introduction to nonlinear learning</span></label>
  			<div class="seven_col">	
  				<div class="description">
    				<p><strong>10.1. </strong> The search for natural laws</p>
    				<p><strong>10.2. </strong> Geometric feature design done ‘by eye’</p>
    				<p><strong>10.3. </strong> Introduction to tools of the trade: kernel, neural network, and tree bases</p>
    				<p><strong>10.4. </strong> The ideal scenario versus reality</p>
    				<p><strong>10.5. </strong> Fixed versus adjustable basis functions and approximation</p>
    				<p><strong>10.6. </strong> The gist of cross-validation</p>
    				<p><strong>10.7. </strong> Which basis works best?</p>
  				</div>
			</div>		
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter11" checked/>   
  			<label for="chapter11"><span>CHAPTER 11. Kernel methods</span></label>
  			<div class="six_col">	
  				<div class="description">
    				<p><strong>11.1. </strong> Motivation and basic examples</p>
    				<p><strong>11.2. </strong> Failure to scale in the dimension of features</p>
    				<p><strong>11.3. </strong> The kernel trick via the fundamental theorem of Linear algebra</p>
    				<p><strong>11.4. </strong> Kernelizing supervised and unsupervised problems</p>
    				<p><strong>11.5. </strong> Kernels as similarity matrices</p>
    				<p><strong>11.6. </strong> The failure to scale with large datasets</p>
  				</div>
			</div>		
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter12" checked/>   
  			<label for="chapter12"><span>CHAPTER 12. Tree-based methods</span></label>
  			<div class="five_col">	
  				<div class="description">
    				<p><strong>12.1. </strong> Recursively defined tree-based functions</p>
    				<p><strong>12.2. </strong> Random forests</p>
    				<p><strong>12.3. </strong> Greedy coordinate descent and the generic booster </p>
    				<p><strong>12.4. </strong> Gradient boosting</p>
    				<p><strong>12.5. </strong> Adaboost and logitboost</p>
  				</div>
			</div>	
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter13" checked/>   
  			<label for="chapter13"><span>CHAPTER 13. Multilayer perceptrons</span></label>
  			<div class="six_col">	
  				<div class="description">
    				<p><strong>13.1. </strong> Computation graphs and the construction of endlessly complex functions</p>
    				<p><strong>13.2. </strong> Automatic differentiation part 2: the backward method (a.k.a. Backpropagation) </p>
    				<p><strong>13.3. </strong> Designing generic deep networks recursively</p>
    				<p><strong>13.4. </strong> Computation graphs, deep networks, and efficient computation</p>
    				<p><strong>13.5. </strong> PCA and the autoencoder</p>
    				<p><strong>13.6. </strong> Cross-validation by regularization, early stopping, and dropout</p>
  				</div>
			</div>
		</div>		

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter14" checked/>   
  			<label for="chapter14"><span>CHAPTER 14. Mathematical Optimization III: Optimization tricks for multilayer perceptrons</span></label>
  			<div class="seven_col">	
  				<div class="description">
    				<p><strong>14.1. </strong> Nonlinear features, feature scaling, and batch-normalization</p>
    				<p><strong>14.2. </strong> Regularization and convexification</p>
    				<p><strong>14.3. </strong> Momentum and gradient descent</p>
    				<p><strong>14.4. </strong> Normalized gradient descent and non-convex functions</p>
    				<p><strong>14.5. </strong> General steepest descent methods</p>
    				<p><strong>14.6. </strong> Stochastic and minibatch methods</p>
    				<p><strong>14.7. </strong> Engineered first order methods for feedforward networks</p>
  				</div>
			</div>	
		</div>

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter15" checked/>   
  			<label for="chapter15"><span>CHAPTER 15. Recurrent Neural Networks (RNNs)</span></label>
  			<div class="seven_col">	
  				<div class="description">
    				<p><strong>15.1. </strong> The ‘knowledge versus data’ trade-off curve</p>
    				<p><strong>15.2. </strong> Common examples of ordered data used in supervised learning</p>
    				<p><strong>15.3. </strong> Recursive sequences and functions, Markov models</p>
    				<p><strong>15.4. </strong> Basic and hidden recursive models</p>
    				<p><strong>15.5. </strong> The simple Recurrent Neural Network  </p>
    				<p><strong>15.6. </strong> Learning long term dependencies, deficiencies of the simple RNN </p>
    				<p><strong>15.7. </strong> Architectures for learning long term dependencies: the identity RNN, LSTM, and GRU</p>
  				</div>
			</div>	
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter16" checked/>   
  			<label for="chapter16"><span>CHAPTER 16. Convolutional Neural Networks (CNNs) - Part I</span></label>
  			<div class="five_col">
  				<div class="description">
    				<p><strong>16.1. </strong> Spatially ordered data, images, and convolutions </p>
    				<p><strong>16.2. </strong> Convolutions and their many applications to signal and image processing</p>
    				<p><strong>16.3. </strong> Histogram Features for real data: convolution and pooling operations</p>
    				<p><strong>16.4. </strong> Learning with fixed convolution kernels</p>
    				<p><strong>16.5. </strong> Learnable kernels and convolutional networks</p>
  				</div>
			</div>				
		</div>

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter17" checked/>   
  			<label for="chapter17"><span>CHAPTER 17. Convolutional Neural Networks (CNNs) - Part II </span></label>
  			<div class="five_col">	
  				<div class="description">
    				<p><strong>17.1. </strong> Classic and modern convolutional architectures</p>
    				<p><strong>17.2. </strong> Structured output regression and localization</p>
    				<p><strong>17.3. </strong> Transfer learning</p>
    				<p><strong>17.4. </strong> Unsupervised learning pre-training</p>
    				<p><strong>17.5. </strong> Adversarial examples and the fragility of convolutional networks</p>
  				</div>
			</div>
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter18" checked/>   
  			<label for="chapter18"><span>CHAPTER 18. Introduction to Reinforcement Learning </span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>18.1. </strong> Fundamental ideas and examples</p>
    				<p><strong>18.2. </strong> The basic Q-Learning algorithm</p>
    				<p><strong>18.3. </strong> Exploration-exploitation trade-off, short-term long-term reward</p>
    				<p><strong>18.4. </strong> Generalizability of Q-Learning</p>
  				</div>
			</div>	
		</div>
		
		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter19" checked/>   
  			<label for="chapter19"><span>CHAPTER 19. Reinforcement Learning in large state spaces</span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>19.1. </strong> Challenges in scaling to large state spaces</p>
    				<p><strong>19.2. </strong> Function approximators, Deep Q-Learning, and memory replay</p>
    				<p><strong>19.3. </strong> Policy gradient method: geometric and probabilistic perspectives</p>
    				<p><strong>19.4. </strong> Model based reinforcement and optimal control	</p>
  				</div>
			</div>		
		</div>

		<div class="chapters-container">
  			<input type="checkbox" name="chapters" id="chapter20" checked/>   
  			<label for="chapter20"><span>CHAPTER 20. Reinforcement Learning in large action spaces </span></label>
  			<div class="four_col">	
  				<div class="description">
    				<p><strong>20.1. </strong> The limits of discretization</p>
    				<p><strong>20.2. </strong> Deep Q-Learning in the continuous action domain</p>
    				<p><strong>20.3. </strong> Actor-Critic methods</p>
    				<p><strong>20.4. </strong> Approximate Q-Learning and the wire-fitting algorithm</p>
  				</div>
			</div>
		</div>		

		
		<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
	
		<!-- uncomment subscription
		<!- - subscription button - ->
  		<form action="https://formspree.io/rzbrhn@gmail.com" method="POST">
    		<input type="email" name="email" placeholder="Enter your email to get notified when new posts are published" onfocus="this.placeholder=''" onblur="this.placeholder='Enter your email to get notified when new posts are published'" autocomplete="off">
    		<button type="submit" value="Send">Subscribe &nbsp;<i class="fa fa-envelope-o"></i></button>
 		</form>	
 		-->
		
		<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

		<script>
			$(document).ready(function(){
				$('.hover').hover(function(){
					$(this).addClass('flip');
					},function(){
						$(this).removeClass('flip');
					});
				});
		</script>
		
	
	</body>
</html>
