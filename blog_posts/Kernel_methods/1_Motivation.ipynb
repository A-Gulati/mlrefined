{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we describe fixed feature kernels, which is a method of representing fixed basis features so that they scale more gracefully when applied to vector valued input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serious practical issue presents itself when applying fixed basis features to vector valued input: even with a moderate sized input dimension $N$, the corresponding dimension\n",
    "$M$ of the transformed features grows rapidly with $N$ and quickly becomes prohibitively\n",
    "large in terms of storage and computation. For example, the precise number $M$ of non-\n",
    "bias features/feature weights of a degree $D$ polynomial of an input with dimension\n",
    "$N$ is $\\left(\\begin{array}{c}\n",
    "N+D\\\\\n",
    "D\n",
    "\\end{array}\\right)-1=\\frac{\\left(N+D\\right)!}{N!D!}-1$. Even if the input dimension is of reasonably small size, for instance $N=100$ or $N=500$, then just the associated degree $D=5$ polynomial feature map of these input dimensions has dimension $M= 96,560,645$ and $M=268,318,178,226$ respectively! In the latter case we cannot even hold the feature vectors in memory on a modern computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding number of transformed features with a Fourier basis/map is even more gargantuan: the degree $D$ Fourier feature map of arbitrary input dimension $N$ has $\\left(2D + 1\\right)^N$ associated/feature weights. When $D=5$ and $N=80$ this is $11^{80}$, a number larger than current estimates of the number of atoms in the visible universe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This crucial issue, of not being able to effectively store high dimensional fixed basis feature transformations, motivates the search for more efficient representations of fixed bases. Here we introduce kernels or kernelized representations of fixed feature transformations, which are clever ways of constructing them that do not require explicit construction of the fixed features themselves. Kernels allow us to avoid this combinatorial storage problem and use fixed features with vector input (at the cost, as we will see, of scaling poorly with the size of a dataset). Additionally they provide a way of generating new fixed feature maps defined solely through such a kernelized representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "197px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
