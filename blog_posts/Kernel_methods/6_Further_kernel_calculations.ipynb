{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further kernel calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelizing various cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we derive the kernelization of the three core classification models: softmax cost/logistic regression, soft-margin SVMs, and the multiclass softmax classifier. Although we will only describe how to kernelize the $\\ell_{2}$ regularizer along with the SVM model, precisely the same argument can be made in combination with either the two or multiclass softmax classifiers. As with the derivation\n",
    "for Least Squares regression shown in Section \\ref{subsec:Kernelizing-cost-functions}\n",
    "here the main tool for kernelizing these models is again the *Fundamental Theorem of Linear Algebra* described in Section \\ref{subsec:The-fundamental-theorem-linear-algebra}. \n",
    "\n",
    "Throughout this Section we will suppose that an arbitrary $M$ dimensional fixed feature vector has been taken of the input of $P$ points $\\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\right)\\right\\} _{p=1}^{P}$\n",
    "giving feature vectors $\\mathbf{f}_{p}=\\left[\\begin{array}{cccc}\n",
    "f_{1}\\left(\\mathbf{x}_{p}\\right) & f_{2}\\left(\\mathbf{x}_{p}\\right) & \\cdots & f_{M}\\left(\\mathbf{x}_{p}\\right)\\end{array}\\right]^{T}$ for each $\\mathbf{x}_{p}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span> Kernelizing two-class softmax classification/logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the softmax perceptron cost function using with fixed feature\n",
    "mapped input is given as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b,\\mathbf{w}\\right)=\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(b+\\mathbf{f}_{p}^{T}\\mathbf{w}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Using the fundamental theorem of linear algebra for any $\\mathbf{w}$\n",
    "we can then write $\\mathbf{w}=\\mathbf{F}\\mathbf{z}+\\mathbf{r}$ where\n",
    "$\\mathbf{F}^{T}\\mathbf{r}=\\mathbf{0}_{P\\times1}$. Making this substitution\n",
    "into the above and simplifying gives\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b,\\mathbf{z}\\right)=\\begin{aligned}\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(b+\\mathbf{f}_{p}^{T}\\mathbf{F}\\mathbf{z}\\right)}\\right)\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "and denoting the kernel matrix $\\mathbf{H}=\\mathbf{F}^{T}\\mathbf{F}$\n",
    "(where $\\mathbf{h}_{p}=\\mathbf{F}^{T}\\mathbf{f}_{p}$ is the $p^{th}$\n",
    "column of $\\mathbf{H}$) we can then write the above in kernelized\n",
    "form as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b,\\mathbf{z}\\right)=\\begin{aligned}\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(b+\\mathbf{h}_{p}^{T}\\mathbf{z}\\right)}\\right)\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This is the kernelized form of logistic regression shown in Table\n",
    "\\ref{tab:kernelized-versions}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span> Kernelizing soft-margin SVM/regularized margin-perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the soft-margin SVM cost/regularized margin-perceptron cost\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b,\\mathbf{w}\\right)=\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{max}^{2}\\left(0,1-y_{p}^{\\,}\\left(b+\\mathbf{f}_{p}^{T}\\mathbf{w}\\right)\\right)+\\lambda\\left\\Vert \\mathbf{w}\\right\\Vert _{2}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Applying the fundamental theorem of linear algebra we may then write\n",
    "$\\mathbf{w}$ as $\\mathbf{w}=\\mathbf{F}\\mathbf{z}+\\mathbf{r}$ where\n",
    "$\\mathbf{F}^{T}\\mathbf{r}=\\mathbf{0}_{P\\times1}$. Substituting this\n",
    "into the cost and noting that then $\\mathbf{w}^{T}\\mathbf{w}=\\left(\\mathbf{F}\\mathbf{z}+\\mathbf{r}\\right)^{T}\\left(\\mathbf{F}\\mathbf{z}+\\mathbf{r}\\right)=\\mathbf{z}^{T}\\mathbf{F}^{T}\\mathbf{F}\\mathbf{z}+\\mathbf{r}^{T}\\mathbf{r}=\\mathbf{z}^{T}\\mathbf{H}\\mathbf{z}+\\left\\Vert \\mathbf{r}\\right\\Vert _{2}^{2}$\n",
    "denoting $\\mathbf{H}=\\mathbf{F}^{T}\\mathbf{F}$ as the kernel matrix\n",
    "we may rewrite the above equivalently as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}g\\left(b,\\mathbf{z},\\mathbf{r}\\right)=\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{max}^{2}\\left(0,1-y_{p}^{\\,}\\left(b+\\mathbf{h}_{p}^{T}\\mathbf{z}\\right)\\right)+\\lambda\\mathbf{z}^{T}\\mathbf{H}\\mathbf{z}+\\lambda\\left\\Vert \\mathbf{r}\\right\\Vert _{2}^{2}\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that since we are aiming to minimize the quantity above over\n",
    "$\\left(b,\\mathbf{z},\\mathbf{r}\\right)$, and since the only term with\n",
    "$\\mathbf{r}$ remaining is $\\left\\Vert \\mathbf{r}\\right\\Vert _{2}^{2}$,\n",
    "that the optimal value of $\\mathbf{r}$ is zero for otherwise the\n",
    "value of the cost function would be larger than necessary. Therefore\n",
    "we can ignore $\\mathbf{r}$ and write the cost function above in kernelized\n",
    "form as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b,\\mathbf{z}\\right)=\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{max}^{2}\\left(0,1-y_{p}^{\\,}\\left(b+\\mathbf{h}_{p}^{T}\\mathbf{z}\\right)\\right)+\\lambda\\mathbf{z}^{T}\\mathbf{H}\\mathbf{z}\n",
    "\\end{equation}\n",
    "\n",
    "as originally shown in Table \\ref{tab:kernelized-versions}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span>  Kernelizing the multiclass softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the multiclass softmax cost function is written as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b_{1},...,b_{C},\\mathbf{w}_{1},...,\\mathbf{w}_{C}\\right)=\\underset{c=1}{\\overset{C}{\\sum}}\\underset{p\\in\\Omega_{c}}{\\sum}\\mbox{log}\\left(1+\\underset{\\underset{j\\neq c}{j=1}}{\\overset{C}{\\sum}}e^{\\left(b_{j}^{\\,}-b_{c}^{\\,}\\right)+\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting each $\\mathbf{w}_{j}$ as $\\mathbf{w}_{j}=\\mathbf{F}\\mathbf{z}_{j}+\\mathbf{r}_{j}$\n",
    "where $\\mathbf{F}^{T}\\mathbf{r}_{j}=\\mathbf{0}_{P\\times1}$ for all\n",
    "$j$ we can rewrite each $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)$\n",
    "term as $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)=\\mathbf{f}_{p}^{T}\\left(\\mathbf{F}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)+\\left(\\mathbf{r}_{j}-\\mathbf{r}_{c}\\right)\\right)=\\mathbf{f}_{p}^{T}\\mathbf{F}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)$.\n",
    "And denoting $\\mathbf{H}=\\mathbf{F}^{T}\\mathbf{F}$ the kernel matrix\n",
    "this we have that $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)=\\mathbf{h}_{p}^{T}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)$, and so the cost may be written eqvuivalently (kernelized) as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b_{1},...,b_{C},\\mathbf{z}_{1},...,\\mathbf{z}_{C}\\right)=\\underset{c=1}{\\overset{C}{\\sum}}\\underset{p\\in\\Omega_{c}}{\\sum}\\mbox{log}\\left(1+\\underset{\\underset{j\\neq c}{j=1}}{\\overset{C}{\\sum}}e^{\\left(b_{j}^{\\,}-b_{c}^{\\,}\\right)+\\mathbf{h}_{p}^{T}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "as shown in Table \\ref{tab:kernelized-versions}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourier kernel calculations - scalar input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Example \\ref{Fourier-kernel} the $\\left(i,j\\right)^{th}$ element\n",
    "of the kernel matrix $\\mathbf{H}$ is given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\mathbf{H}_{ij}=2\\underset{m=1}{\\overset{D}{\\sum}}\\mbox{cos}\\left(2\\pi mx_{i}\\right)\\mbox{cos}\\left(2\\pi mx_{j}\\right)+\\mbox{sin}\\left(2\\pi mx_{i}\\right)\\mbox{sin}\\left(2\\pi mx_{j}\\right)\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Writing this using the complex exponential notation (see Exercise\n",
    "\\ref{exercise-complex-Fourier-representation}) we have equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\mathbf{H}_{ij}=\\underset{m=-D}{\\overset{D}{\\sum}}e^{2\\pi im\\left(x_{i}-x_{j}\\right)}-1\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "If $x_{i}-x_{j}$ is an integer then $e^{2\\pi im\\left(x_{i}-x_{j}\\right)}=1$\n",
    "and so clearly the above sums to $2D$. Supposing this is not the\n",
    "case, examining the summation alone we may write\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{m=-D}{\\overset{D}{\\sum}}e^{2\\pi im\\left(x_{i}-x_{j}\\right)}=e^{-2\\pi iD\\left(x_{i}-x_{j}\\right)}\\underset{m=0}{\\overset{2D}{\\sum}}e^{2\\pi im\\left(x_{i}-x_{j}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "Now, the sum on the right hand side above is a geometric series, thus\n",
    "we have the above is equal to\n",
    "\n",
    "\\begin{equation}\n",
    "e^{-2\\pi iD\\left(x_{i}-x_{j}\\right)}\\frac{1-e^{2\\pi i\\left(x_{i}-x_{j}\\right)\\left(2D+1\\right)}}{1-e^{2\\pi i\\left(x_{i}-x_{j}\\right)}}=\\frac{\\mbox{sin}\\left(\\left(2D+1\\right)\\pi\\left(x_{i}-x_{j}\\right)\\right)}{\\mbox{sin}\\left(\\pi\\left(x_{i}-x_{j}\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where final equality follows from the definition of the complex exponential.\n",
    "Because in the limit as $t$ approaches any integer value $\\frac{\\mbox{sin}\\left(\\left(2D+1\\right)\\pi t\\right)}{\\mbox{sin}\\left(\\pi t\\right)}=2D+1$,\n",
    "which one can show using L'Hospital's rule from basic calculus, we\n",
    "may therefore generally write in conclusion that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H}_{ij}=\\frac{\\mbox{sin}\\left(\\left(2D+1\\right)\\pi\\left(x_{i}-x_{j}\\right)\\right)}{\\mbox{sin}\\left(\\pi\\left(x_{i}-x_{j}\\right)\\right)}-1\n",
    "\\end{equation}\n",
    "\n",
    "where at integer values of the input it is defined by the associated limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier kernel calculations - vector input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Like the multidimensional polynomial basis element (see Footnote \\ref{fn:N-dim-input-poly/fourier})\n",
    "with the complex exponential notation for a general $N$ dimensional\n",
    "input each Fourier basis element takes the form $f_{\\mathbf{m}}\\left(\\mathbf{x}\\right)=e^{2\\pi im_{1}x_{1}}e^{2\\pi im_{2}x_{2}}\\cdots e^{2\\pi im_{N}x_{N}}=e^{2\\pi i\\mathbf{m}^{T}\\mathbf{x}}$\n",
    "where $\\mathbf{m}=\\left[\\begin{array}{cccc}\n",
    "m_{1} & m_{2} & \\cdots & m_{N}\\end{array}\\right]^{T}$, a product of one dimensional basis elements. Further a 'degree $D$'\n",
    "sum contains all such basis elements where $-D\\leq m_{1},\\,m_{2},\\,\\cdots,\\,m_{N}\\leq D$,\n",
    "and one may deduce that there are $M=\\left(2D+1\\right)^{N}-1$ non\n",
    "constant basis elements in this sum. \n",
    "\n",
    "The the corresponding $\\left(i,j\\right)$th entry of the kernel matrix\n",
    "in this instance takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H}_{ij}=\\mathbf{f}{}_{i}^{T}\\overline{\\mathbf{f}_{j}}=\\left(\\underset{-D\\leq m_{1},\\,m_{2},\\,\\cdots,\\,m_{N}\\leq D}{\\sum}e^{2\\pi i\\mathbf{m}^{T}\\left(\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right)}\\right)-1\n",
    "\\end{equation}\n",
    "\n",
    "Since $e^{a+b}=e^{a}e^{b}$ we may write each summand above as $e^{2\\pi i\\mathbf{m}^{T}\\left(\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right)}=\\underset{n=1}{\\overset{N}{\\prod}}e^{2\\pi im_{n}\\left(x_{in}-x_{jn}\\right)}$,\n",
    "and the entire summation as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{-D\\leq m_{1},\\,m_{2},\\,\\cdots,\\,m_{N}\\leq D}{\\sum}\\,\\,\\,\\underset{n=1}{\\overset{N}{\\prod}}e^{2\\pi im_{n}\\left(x_{in}-x_{jn}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "Finally one can show that the above can be written simply as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{-D\\leq m_{1},\\,m_{2},\\,\\cdots,\\,m_{N}\\leq D}{\\sum}\\,\\,\\,\\underset{n=1}{\\overset{N}{\\prod}}e^{2\\pi im_{n}\\left(x_{in}-x_{jn}\\right)}=\\underset{n=1}{\\overset{N}{\\prod}}\\left(\\underset{m=-D}{\\overset{D}{\\sum}}e^{2\\pi im\\left(x_{in}-x_{jn}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Since we already have that $\\underset{m=-D}{\\overset{D}{\\sum}}e^{2\\pi im\\left(x_{in}-x_{jn}\\right)}=\\frac{\\sin\\left(\\left(2D+1\\right)\\pi\\left(x_{in}-x_{jn}\\right)\\right)}{\\sin\\left(\\pi\\left(x_{in}-x_{jn}\\right)\\right)}$,\n",
    "the $\\left(i,j\\right)$th entry of the kernel matrix can easily be\n",
    "calculated as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H}_{ij}=\\underset{n=1}{\\overset{N}{\\prod}}\\frac{\\mbox{sin}\\left(\\left(2D+1\\right)\\pi\\left(x_{in}-x_{jn}\\right)\\right)}{\\mbox{sin}\\left(\\pi\\left(x_{in}-x_{jn}\\right)\\right)}-1\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "197px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
