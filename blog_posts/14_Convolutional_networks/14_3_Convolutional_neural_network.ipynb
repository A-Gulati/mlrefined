{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.3 Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code cell won't be shown in the HTML version of this notebook.\n",
    "# import autograd functionalities\n",
    "import autograd.numpy as np\n",
    "from autograd import grad as compute_grad   \n",
    "\n",
    "# import plotting library and other necessities\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# import general libraries\n",
    "import copy\n",
    "from datetime import datetime \n",
    "\n",
    "#this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries.deeplearning_library_v1 import superlearn_setup\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we discussed in full detail how to use the convolution operation to extract features based on the edge content of input images. More elaborate variations of this idea have been used effectively in practice (e.g., HoG [[1]](#bib_cell) and SIFT [[2]](#bib_cell) features) particularly for object detection, where in all cases features are computed using fixed (i.e., pre-defined) kernels. In general, employing this breed of 'engineered' features in place of raw pixel values (see Figure 1) considerably improves the overall performance of supervised/unsupervised learners, as we confirmed empirically in Section 14.2 via a simple face detection experiment.\n",
    "\n",
    "A simple Convolutional Neural Network (CNN), as depicted in the bottom row of Figure 1, is different from the architectures we have seen so far (middle row of Figure 1) in that with CNNs, in addition to the tunable weights of the supervised/unsupervised learner, we also learn the convolutional kernels simultaneously using our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../../mlrefined_images/convnet_images/raw_fixed_CNN.png\" width=\"80%\" height=\"auto\"/>\n",
    "<figcaption> <strong>Figure 1:</strong> <em> Three simple pipelines for machine learning tasks involving images. (top row) Using raw pixels values as input features generally leads to subpar results. (middle row) A fixed convolutional layer (consisting of convolution, ReLU, and pooling modules) is instered between the input image and the MLP module. This is the pipeline used in Section 14.2. (bottom row) With CNNs the kernels in the convolutional layer are also tuned along with the MLP weights. The modules involving tunable weights are colored yellow. </em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall from our discussion of multilayer perceptrons that we can write the ```model``` associated with a general $L$ layer MLP as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\mathbf{w}\\right) = \\mathbf{w}_{L+1}^T\\mathring{\\mathbf{f}}^{(L)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathring{\\mathbf{f}}^{(L)}$, as discussed in Section 13.1, is formed by tacking a $1$ on top of the feature transformation  \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{f}^{(L)} = \\mathbf{a}\\left(\\mathbf{W}_{L}^T\\mathring{\\mathbf{a}}\\left(\\mathbf{W}_{L-1}^T\\,\\mathring{\\mathbf{a}}\\left( \\cdots \\mathbf{W}_{1}^T\\mathring{\\mathbf{x}}\\right) \\right)\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, to write the ```model``` associated with a simple CNN algebraically, all we need to is replace the input $\\mathbf{x}$ in Equation (2) with the output of the convolutional layer given as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{equation}\n",
    "\\text{pool}\\left(\\text{max}\\left(0,\\mathbf{W}_{conv}*\\mathbf{x}\\right)\\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{W}_{conv}$ represents all convolutional kernel weights to be tuned together with MLP weights $\\mathbf{W}_{1},\\ldots,\\mathbf{W}_{L}$ and $\\mathbf{w}_{L+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That the convolutional kernels are learned to the data with CNNs implies we should intuitively expect them to outperform their fixed kernel counterparts - a hypothesis we now put to test through a simple experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1. </span>  CNN vs. fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, the pooling module was introduced first in Section 14.2 in order primarily to reduce the dimension of multiple convolutional feature maps, one produced per each convolutional kernel. This reduction in size is due to the lack of padding with pooling, but more importantly the use of stride values greater than $1$. In the spirit of simplicity, it is worthwhile to explore whether we can achieve dimension reduction during convolution by choosing stride values of greater than $1$, thereby completely forgoing the pooling process.                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 2. </span> To pool or not to pool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bib_cell'></a>\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In\n",
    "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society\n",
    "Conference on, volume 1, pp. 886–893. IEEE, 2005\n",
    "\n",
    "[2] David G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2) 91–110, 2004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
