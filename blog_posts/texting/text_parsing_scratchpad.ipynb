{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in an english text dataset\n",
    "\n",
    "Here I've used the text from the AMAZING book \"War of the Worlds\" by H.G. Wells.  Note: to simplify things we are going to - when we load in the text - convert all characters to *lower case*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in text dataset\n",
    "text = open(\"war_of_the_worlds.txt\").read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the first chunk of characters from this text to see what it looks liie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1504\\\\cocoasubrtf830\\n{\\\\fonttbl\\\\f0\\\\fmodern\\\\fcharset0 courier;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;\\\\red0\\\\green0\\\\blue0;}\\n{\\\\*\\\\expandedcolortbl;;\\\\cssrgb\\\\c0\\\\c0\\\\c0;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\deftab720\\n\\\\pard\\\\pardeftab720\\\\sl280\\\\partightenfactor0\\n\\n\\\\f0\\\\fs24 \\\\cf2 \\\\expnd0\\\\expndtw0\\\\kerning0\\n\\\\outl0\\\\strokewidth0 \\\\strokec2 the project gutenberg ebook of the war of the worlds, by h. g. wells\\\\\\n\\\\\\nthis ebook is for the use of anyone anywhere at no cost and with\\\\\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\\\\\nre-use it under the terms of the project gutenberg license included\\\\\\nwith this ebook or online at www.gutenberg.net\\\\\\n\\\\\\n\\\\\\ntitle: the war of the worlds\\\\\\n\\\\\\nauthor: h. g. wells\\\\\\n\\\\\\nrelease date: july, 1992 [ebook #36]\\\\\\n[most recently updated october 1, 2004]\\\\\\n\\\\\\nlanguage: english\\\\\\n\\\\\\n\\\\\\n*** start of this project gutenberg ebook the war of the worlds ***\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\nthe war of the worlds\\\\\\n\\\\\\nby h. g. wells [1898]\\\\\\n\\\\\\n\\\\\\n     but who shall dwell in these worlds if they be\\\\\\n     inhabited? .  .  .  are we or they lords of the\\\\\\n     world? .  .  .  and how are all things made for man?--\\\\\\n          kepler (quoted in the anatomy of melancholy)\\\\\\n\\\\\\n\\\\\\n\\\\\\nbook one\\\\\\n\\\\\\nthe coming of the martians\\\\\\n\\\\\\n\\\\\\n\\\\\\nchapter one\\\\\\n\\\\\\nthe eve of the war\\\\\\n\\\\\\n\\\\\\nno one would have believed in the last years of the nineteenth\\\\\\ncentury that this world was being watched keenly and closely by\\\\\\nintelligences greater than man's and yet as mortal as his own; that as\\\\\\nmen busied themselves about their various concerns they were\\\\\\nscrutinised and studied, perhaps almost as narrowly as a man with a\\\\\\nmicroscope might scrutinise the transient creatures that swarm and\\\\\\nmultiply in a drop of water.  with infinite complacency men went to\\\\\\nand fro over this globe about their little affairs, serene in their\\\\\\nassurance of their empire over matter.  it is possible that the\\\\\\ninfusoria under the microscope do the same.  no one gave a thought to\\\\\\nthe older wo\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 2000 characters from text\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah - tons of junk here.  From what I can see we need to\n",
    "\n",
    "- cut first hundred or so characters of junk\n",
    "\n",
    "\n",
    "- remove tags like e.g., 'newline' tag '\\n' \n",
    "\n",
    "\n",
    "We need to throw out all these markup tags - probably some other nonsense characters too.  But lets start with tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cut out first chunk of giberish text\n",
    "text = text[947:]\n",
    "\n",
    "# remove some obvious tag-related gibberish throughout\n",
    "characters_to_remove = ['0','1','2','3','4','5','6','7','8','9','_','[',']','}','.  .  .','\\\\']\n",
    "for i in characters_to_remove:\n",
    "    text = text.replace(i,'')\n",
    "    \n",
    "# some gibberish that looks like it needs to be replaced with a ' '\n",
    "text = text.replace('\\n',' ')\n",
    "text = text.replace('\\r',' ')\n",
    "text = text.replace('--',' ')\n",
    "text = text.replace(',,',' ')\n",
    "text = text.replace('   ',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visually examine the first chunk of text now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the war of the worlds  by h. g. wells   but who shall dwell in these worlds if they be  inhabited? are we or they lords of the  world? and how are all things made for man?    kepler (quoted in the anatomy of melancholy)  book one  the coming of the martians  chapter one  the eve of the war no one would have believed in the last years of the nineteenth century that this world was being watched keenly and closely by intelligences greater than man's and yet as mortal as his own; that as men busied themselves about their various concerns they were scrutinised and studied, perhaps almost as narrowly as a man with a microscope might scrutinise the transient creatures that swarm and multiply in a drop of water.  with infinite complacency men went to and fro over this globe about their little affairs, serene in their assurance of their empire over matter.  it is possible that the infusoria under the microscope do the same.  no one gave a thought to the older worlds of space as sources of human danger, or thought of them only to dismiss the idea of life upon them as impossible or improbable.  it is curious to recall some of the mental habits of those departed days.  at most terrestrial men fancied there might be other men upon mars, perhaps inferior to themselves and ready to welcome a missionary enterprise.  yet across the gulf of space, minds that are to our minds as ours are to those of the beasts that perish, intellects vast and cool and unsympathetic, regarded this earth with envious eyes, and slowly and surely drew their plans against us.  and early in the twentieth century came the great disillusionment.  the planet mars, i scarcely need remind the reader, revolves about the sun at a mean distance of miles, and the light and heat it receives from the sun is barely half of that received by this world. it must be, if the nebular hypothesis has any truth, older than our world; and long before this earth ceased to be molten, life upon its surface must have begun its cours\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right - that looks much better.  If we so desire we can do even more pre-processing - remove unwanted punctuation, perhaps strange / rare words, etc,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the text into distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our text pre-processed we can now convert each unique word of the text into a unique number, so that we can then e.g., create a Markov model for word-based text generation.\n",
    "\n",
    "First - we use the [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html) functionality from scikit learn to split the text into its distinct words.  We can then find all of the unique words in the text, assign a number to each unique word, and then convert all words of the text into a corresponding sequence of numbers.  We'll do this one step at a time below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load in the ``CountVectorizer``, load in the text and compute its unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in function from scikit learn that \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([text])\n",
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "# get all unique words in input corpus\n",
    "tokens = analyze(text)\n",
    "unique_words = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many total words, unique words, and total number of characters in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text dataset has 57640 total words in it\n",
      "this text dataset has 6717 unique words in it\n",
      "this text dataset has 338645 characters in it\n"
     ]
    }
   ],
   "source": [
    "# total number of words, unique words, and characters in text\n",
    "print ('this text dataset has ' + str(len(tokens)) + ' total words in it')\n",
    "print ('this text dataset has ' + str(len(unique_words)) + ' unique words in it')\n",
    "print ('this text dataset has ' + str(len(text)) + ' characters in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visually examine the first chunk of unique words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6717 unique words in this text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abandoned',\n",
       " 'abandoning',\n",
       " 'abart',\n",
       " 'abbey',\n",
       " 'abbreviated',\n",
       " 'abiding',\n",
       " 'ability',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'abounded',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique words are in the text?\n",
    "print ('there are ' + str(len(unique_words)) + ' unique words in this text')\n",
    "\n",
    "# examine the first 20 unique words\n",
    "unique_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first few 'tokens' - this is jargon for the text broken into individual words.  A 'token' is just a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'war',\n",
       " 'of',\n",
       " 'the',\n",
       " 'worlds',\n",
       " 'by',\n",
       " 'wells',\n",
       " 'but',\n",
       " 'who',\n",
       " 'shall',\n",
       " 'dwell',\n",
       " 'in',\n",
       " 'these',\n",
       " 'worlds',\n",
       " 'if',\n",
       " 'they',\n",
       " 'be',\n",
       " 'inhabited',\n",
       " 'are',\n",
       " 'we']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first chunk of tokens from the text\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look fine - so now we can make a unique number per unique word.  It naturally makes sense to use a ``dictionary`` for this sort of thing.  Here we'll make two dictionaries - one that converts the unique words to unique numbers, and another that converts the unique numbers back into unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique nums to map words too\n",
    "unique_nums = np.arange(len(unique_words))\n",
    "\n",
    "# this dictionary is a function mapping each unique word to a unique integer\n",
    "words_to_keys = dict((i, n) for (i,n) in zip(unique_words,unique_nums))\n",
    "\n",
    "# this dictionary is a function mapping each unique integer to a unique word\n",
    "keys_to_words = dict((i, n) for (i,n) in zip(unique_nums,unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sure that the two dictionaries mirror each other by printing out a random word / number from each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'albeit'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_to_words[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_keys['albeit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right - makes sense.  With our unique word --> to unique number conversion dictionary created we can now shove the entire text through it and convert it to a sequence of numbers.  The jargon for the sequence of numbers used to represent a sequence of words is ``keys``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all of our tokens (words) to keys\n",
    "keys = [words_to_keys[a] for a in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out the first chunk of tokens, and then shove the keys back through our converter to check that indeed the keys map back to each word properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'war', 'of', 'the', 'worlds', 'by', 'wells', 'but', 'who', 'shall', 'dwell', 'in', 'these', 'worlds', 'if', 'they', 'be', 'inhabited', 'are', 'we']]\n"
     ]
    }
   ],
   "source": [
    "# print first chunk of words, and first chunk of keys mapped back to words\n",
    "print ([tokens[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'war', 'of', 'the', 'worlds', 'by', 'wells', 'but', 'who', 'shall', 'dwell', 'in', 'these', 'worlds', 'if', 'they', 'be', 'inhabited', 'are', 'we']\n"
     ]
    }
   ],
   "source": [
    "# confirm that first chunk of keys maps correctly back to words \n",
    "print([keys_to_words[keys[i]] for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5910, 6457, 3926, 5910, 6659, 716, 6528, 711, 6578, 5119, 1738, 2928, 5928, 6659, 2870, 5929, 409, 2995, 240, 6499]\n"
     ]
    }
   ],
   "source": [
    "# print first chunk of keys\n",
    "print ([keys[i] for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!  Seems to make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the text into distinct characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big picture\n",
    "\n",
    "- Make sure to pre-process in the same way we do in the 'words' version of things.\n",
    "\n",
    "\n",
    "- With the pre-processed text we just need to create dictionaries to convert each unique character to a unique number, and each number back to its unique character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets print out the number of characters, unique characters, and the unique characters themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 338645 total number of characters\n",
      "this corpus has 38 unique characters\n",
      "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique characters in the text\n",
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"this corpus has \" +  str(len(text)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(unique_chars)) + \" unique characters\")\n",
    "print (unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique number range to map characters too\n",
    "unique_nums = np.arange(len(unique_chars))\n",
    "\n",
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_keys = dict((i, n) for (i,n) in zip(unique_chars,unique_nums))\n",
    "\n",
    "# this dictionary is a function mapping each unique integer to a unique character\n",
    "keys_to_chars = dict((i, n) for (i,n) in zip(unique_nums,unique_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test out our dictionary converters to make sure they work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all of our characters to keys\n",
    "keys = [chars_to_keys[a] for a in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 19, 16, 0, 34, 12, 29, 0, 26, 17, 0, 31, 19, 16, 0, 34, 26, 29, 23, 15]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'w', 'a', 'r', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', 's']\n"
     ]
    }
   ],
   "source": [
    "# confirm that first chunk of keys maps correctly back to words \n",
    "print([text[i] for i in range(21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'w', 'a', 'r', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', 's']\n"
     ]
    }
   ],
   "source": [
    "# confirm that first chunk of keys maps correctly back to words \n",
    "print([keys_to_chars[keys[i]] for i in range(21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
