{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class perceptron reduces to two-class perceptron when $C = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $C=2$ the multi-class perceptron cost in equation (4) reduces to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(0)},\\,\\mathbf{w}_{\\mathstrut}^{(0)},\\,w_0^{(1)},\\,\\mathbf{w}_{\\mathstrut}^{(1)} \\right) = \\sum_{p = 1}^P \\left[\\,{\\text{max}} \\left(w_0^{(0)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(0)},\\,w_0^{(1)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(1)}\\right) - \\left(w_0^{(y_p)} + \\mathbf{x}_{p}^T\\mathbf{w}_{\\mathstrut}^{(y_p)}\\right) \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, using the following equality for any real values $a$, $b$, and $c$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\text{max}} \\left(a,b\\right) - c = {\\text{max}} \\left(a-c,b-c\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be written equivalently as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(w_0^{(0)},\\,\\mathbf{w}_{\\mathstrut}^{(0)},\\,w_0^{(1)},\\,\\mathbf{w}_{\\mathstrut}^{(1)} \\right) = \\sum_{p = 1}^P {\\text{max}} \\left(w_0^{(0)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(0)}- \\left(w_0^{(y_p)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(y_p)}\\right), \\,w_0^{(1)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(1)}- \\left(w_0^{(y_p)} + \\mathbf{x}_{p}^T\\mathbf{w}^{(y_p)}\\right)\\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the summands according to their labels, we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(0)},\\,\\mathbf{w}_{\\mathstrut}^{(0)},\\,w_0^{(1)},\\,\\mathbf{w}_{\\mathstrut}^{(1)} \\right) = \\sum_{p:\\, y_p = 0} {\\text{max}} \\left(0, \\,w_0^{(1)}-w_0^{(0)} + \\mathbf{x}_{p}^T\\left(\\mathbf{w}^{(1)}-\\mathbf{w}^{(0)}\\right)\\right)\\\\\n",
    "+ \\sum_{p:\\, y_p = 1} {\\text{max}} \\left(0, \\,w_0^{(0)}-w_0^{(1)} + \\mathbf{x}_{p}^T\\left(\\mathbf{w}^{(0)}-\\mathbf{w}^{(1)}\\right)\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-assigning the labels $y_p=0 \\rightarrow y_p=-1$ and $y_p=1 \\rightarrow y_p=+1$ to match the label values we used in deriving the two-class perceptron, we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(-1)},\\,\\mathbf{w}_{\\mathstrut}^{(-1)},\\,w_0^{(+1)},\\,\\mathbf{w}_{\\mathstrut}^{(+1)} \\right) = \\sum_{p:\\, y_p = -1} {\\text{max}} \\left(0, \\,w_0^{(+1)}-w_0^{(-1)} + \\mathbf{x}_{p}^T\\left(\\mathbf{w}^{(+1)}-\\mathbf{w}^{(-1)}\\right)\\right) \\\\+ \\sum_{p:\\, y_p = +1} {\\text{max}} \\left(0, \\,w_0^{(-1)}-w_0^{(+1)} + \\mathbf{x}_{p}^T\\left(\\mathbf{w}^{(-1)}-\\mathbf{w}^{(+1)}\\right)\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $w_0=w_0^{(+1)}-w_0^{(-1)}$ and $\\mathbf{w}=\\mathbf{w}^{(+1)}-\\mathbf{w}^{(-1)}$, the above can be written as  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0,\\mathbf{w} \\right) = \\sum_{p:\\, y_p = -1} {\\text{max}} \\left(0, \\,w_0 + \\mathbf{x}_{p}^T\\mathbf{w}\\right) + \\sum_{p:\\, y_p = +1} {\\text{max}} \\left(0, \\,-w_0 - \\mathbf{x}_{p}^T\\mathbf{w}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written more compactly to arrive at the familiar two-class perceptron cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0,\\mathbf{w} \\right) = \\sum_{p=1}^P {\\text{max}} \\left(0, -y_p\\left(w_0 + \\mathbf{x}_{p}^T\\mathbf{w}\\right)\\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class perceptron and softmax costs are convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we always have that:\n",
    "    \n",
    "**I.** Addition of two (or more) convex functions is always convex.\n",
    "\n",
    "**II.** Linear and affine functions are convex.\n",
    "\n",
    "**III.** The max, exponential, and negative logarithm functions are all convex.\n",
    "\n",
    "**IV.** Composition of two convex functions remains convex.\n",
    "\n",
    "Each of the statements above can be verified easily using the following definition of convexity:\n",
    "\n",
    "A function g is convex if and only if for all $\\mathbf{w}_1$ and $\\mathbf{w}_2$ in the domain of g and all $\\lambda \\in \\left[0, 1\\right]$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\lambda \\mathbf{w}_1+\\left(1-\\lambda\\right) \\mathbf{w}_2\\right)\\leq \\lambda g\\left(\\mathbf{w}_1\\right)+\\left(1-\\lambda\\right) g\\left(\\mathbf{w}_2\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these four statements at hand, it is straight-forward to prove convexity of multi-class perceptron and softmax cost functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression reduces to two-class logistic regression when $C = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $C=2$ the multi-class softmax cost in equation (12) reduces to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(0)},\\,\\mathbf{w}_{\\mathstrut}^{(0)},\\,w_0^{(1)},\\,\\mathbf{w}_{\\mathstrut}^{(1)} \\right) = \\sum_{p = 1}^P \\text{log}\\left(\\frac{ e^{ w_0^{(0)} + \\mathbf{x}_{p}^T\\mathbf{w}_{\\mathstrut}^{(0)}} +e^{ w_0^{(1)} + \\mathbf{x}_{p}^T\\mathbf{w}_{\\mathstrut}^{(1)}}  }{ e^{ w_0^{(y_p)} + \\mathbf{x}_{p}^T\\mathbf{w}_{\\mathstrut}^{(y_p)}} }\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the summands according to their labels, we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(0)},\\,\\mathbf{w}_{\\mathstrut}^{(0)},\\,w_0^{(1)},\\,\\mathbf{w}_{\\mathstrut}^{(1)} \\right) = \\sum_{p:\\, y_p = 1} \\text{log}\\left( 1 + e^{w_0^{(1)}-w_0^{(0)} + \\mathbf{x}_{p}^T \\left(\\mathbf{w}^{(1)}-\\mathbf{w}^{(0)}\\right)} \\right) \\\\+ \\sum_{p:\\, y_p = 2} \\text{log}\\left( 1 + e^{w_0^{(0)}-w_0^{(1)} + \\mathbf{x}_{p}^T \\left(\\mathbf{w}^{(0)}-\\mathbf{w}^{(1)}\\right)} \\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-assigning the labels $y_p=0 \\rightarrow y_p=-1$ and $y_p=1 \\rightarrow y_p=+1$ to match the label values we used in deriving the two-class softmax cost previously, we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0^{(-1)},\\,\\mathbf{w}^{(-1)},\\,w_0^{(+1)},\\,\\mathbf{w}_{\\mathstrut}^{(+1)} \\right)  = \\sum_{p:\\, y_p = -1} \\text{log}\\left( 1 + e^{w_0^{(+1)}-w_0^{(-1)} + \\mathbf{x}_{p}^T \\left(\\mathbf{w}^{(+1)}-\\mathbf{w}^{(-1)}\\right)} \\right) \\\\+ \\sum_{p:\\, y_p = +1} \\text{log}\\left( 1 + e^{w_0^{(-1)}-w_0^{(+1)} + \\mathbf{x}_{p}^T \\left(\\mathbf{w}^{(-1)}-\\mathbf{w}^{(+1)}\\right)} \\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $w_0=w_0^{(+1)}-w_0^{(-1)}$ and $\\mathbf{w}=\\mathbf{w}^{(+1)}-\\mathbf{w}^{(-1)}$, the above can be written as  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0,\\mathbf{w} \\right)  = \\sum_{p:\\, y_p = -1} \\text{log}\\left( 1 + e^{w_0 + \\mathbf{x}_{p}^T\\mathbf{w}} \\right) + \\sum_{p:\\, y_p = +1} \\text{log}\\left( 1 + e^{-w_0 - \\mathbf{x}_{p}^T \\mathbf{w}} \\right) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written more compactly to arrive at the familiar two-class softmax cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g\\left(w_0,\\mathbf{w} \\right) = \\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(w_0+\\mathbf{x}_{p}^{T}\\mathbf{w}\\right)}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3  Weighted multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "\n",
    "# demos for this notebook\n",
    "regress_plotter = superlearn.lin_regression_demos\n",
    "optimizers = optlib.optimizers\n",
    "static_plotter = optlib.static_plotter.Visualizer()\n",
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "\n",
    "# import autograd functionality to bulid function's properly for optimizers\n",
    "import autograd.numpy as np\n",
    "\n",
    "# import timer\n",
    "from datetime import datetime \n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Remember: while the base of all regressions is to make the following hold by tuning $\\mathbf{w}$\n",
    " \n",
    " \n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x}_p,\\mathbf{w}\\right) \\approx y_p\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and for two-class classification (using $\\pm 1$ labels)\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sign}\\left(\\text{model}\\left(\\mathbf{x}_p,\\mathbf{w}\\right)\\right) \\approx y_p\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "- The analagous desire for multiclass classification is given by the *fusion rule*\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "y_p =  \\underset{j \\,=\\, 0,...,C-1}{\\text{argmax}}\\,\\text{model}_j\\left(\\mathbf{x}_p,\\mathbf{w}^{\\left(j\\right)}\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here we have a `model` for each of our $C$ classifiers (in the simplest instance these are linear)\n",
    "\n",
    "\n",
    "- When we use a shared architecture / model $\\mathbf{w}^{\\left(j\\right)}$ denotes the weights of the classifier-unique linear combination\n",
    "\n",
    "\n",
    "- Weighting here works just as with regression / two-class classification, e.g., weighting a multiclass softmax / logistic regression cost looks like\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = -\\frac{1}{P}\\sum_{p = 1}^P \\text{log}\\left(\\frac{e^{\\text{model}_{y_p}\\left(\\mathbf{x}_p,\\mathbf{w}^{\\left(y_p\\right)}\\right)}}{\\sum_{j = 0}^{C-1} e^{\\text{model}_{j}\\left(   \\mathbf{x}_p,\\mathbf{w}^{\\left(j\\right)}    \\right)}  }\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The weighted version looks precisely as previous - an individual weight $\\beta_p$ controls the contribution of the $p^{th}$ point in the summand \n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = -\\frac{1}{P}\\sum_{p = 1}^P \\beta_p \\, \\text{log}\\left(\\frac{e^{\\text{model}_{y_p}\\left(\\mathbf{x}_p,\\mathbf{w}^{\\left(y_p\\right)}\\right)}}{\\sum_{j = 0}^{C-1} e^{\\text{model}_{j}\\left(   \\mathbf{x}_p,\\mathbf{w}^{\\left(j\\right)}    \\right)}  }\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- This is done for the same reasons listed with weighted two-class classification - commonly to deal with large class imbalances\n",
    "\n",
    "\n",
    "- Our weightings determine *how important each datapoint is* in the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# weighted multiclass softmax\n",
    "def multiclass_softmax(self,w,x,y,beta,iter):     \n",
    "    # get subset of points\n",
    "    x_p = x[:,iter]\n",
    "    y_p = y[:,iter]\n",
    "    beta_p = beta[:,iter]\n",
    "\n",
    "    # pre-compute predictions on all points\n",
    "    all_evals = model(x_p,w)\n",
    "\n",
    "    # compute softmax across data points\n",
    "    a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n",
    "\n",
    "    # compute cost in compact form using numpy broadcasting\n",
    "    b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
    "    cost = np.sum(beta_p*(a - b))\n",
    "\n",
    "    # return average\n",
    "    return cost/float(np.size(y_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
