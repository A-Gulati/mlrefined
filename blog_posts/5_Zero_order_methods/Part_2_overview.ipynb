{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Fundamentals of Mathematical Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this brief discussion we motivate the study of **mathematical optimization**, the collection of methods built on basic calculus by which we determine proper parameters for machine learning / deep learning models. When viewed geometrically the pursuit of proper parameters is also the search for the lowest point - or minimum - of a machine learning model's associated cost function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical optimization and machine learning / deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine learning / deep learning learning problem has parameters that must be tuned properly to ensure optimal learning. For example, there are two parameters that must be properly tuned in the case of a simple linear regression - when fitting a line to a scatter of data: the slope and intercept of the linear model.  \n",
    "\n",
    "These two parameters are tuned by forming what is called a *cost function* or *loss function*.  This is a continuous function in both parameters - that measures how well the linear model fits a dataset given a value for its slope and intercept. The proper tuning of these parameters via the cost function corresponds geometrically to finding the values for the parameters that make the cost function as small as possible or, in other words, *minimize* the cost function.  The image below - taken from [[1]]((#references)) -  illustrates how choosing a set of parameters higher on the cost function results in a corresponding line fit that is poorer than the one corresponding to parameters at the lowest point on the cost surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../mlrefined_images/math_optimization_images/bigpicture_regression_optimization.png\" width=500 height=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same idea holds true for regression with higher dimensional input, as well as classification where we must properly tune parameters to *separate* classes of data.\n",
    "Again, the parameters minimizing an associated cost function provide the best classification result. This is illustrated for classification below - again taken from [[1]](#references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../mlrefined_images/math_optimization_images/bigpicture_classification_optimization.png\" width=500 height=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning of these parameters require the *minimization of a cost function* can be formally written as follows.  For a generic function $g(\\mathbf{w})$ taking in a general $N$ dimensional input $\\mathbf{w}$ the problem of finding the particular point $\\mathbf{v}$ where $g$ attains its smallest value is written formally as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}}{\\mbox{minimize}}\\,\\,\\,\\,g\\left(\\mathbf{w}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "As detailed in our series on the *vital elements of calculus*, the first order optimality condition characterizes solutions to this problem.  However because these conditions can very rarely be solved 'by hand' we must rely on algorithmic techniques for finding function minima (or at the very least finding points close to them).  In this part of the text we examine many algorithmic methods of *mathematical optimization*, which aim to do just this.\n",
    "\n",
    "> The tools of mathematical optimization are designed to minimize cost functions.  When applied to learning problems this corresponds to properly tuning the parameters of a learning model.  Mathematical optimization is the workhorse of machine learning / deep learning, playing a role in virtually every learning problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
