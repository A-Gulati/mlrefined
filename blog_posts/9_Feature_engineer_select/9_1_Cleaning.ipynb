{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Principles of Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 Data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine learning paradigm requires that the data we deal with consists strictly of *numerical values*, however raw data does not always come pre-packaged in this manner.  In in this Section we briefly describe two of the most common ways input features of a dataset may violate this numerical necessity: either when a dataset has *missing input values*, or when it consists of *categorical features*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1  Filtering out irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- functionally, if all elements of a given input feature are constant the feature has no discriminative power\n",
    "\n",
    "\n",
    "- mathematically this simply result in all input models essentially adding a constant $c$ to them\n",
    "\n",
    "\\begin{equation}\n",
    "model + x_{p,n}w_n = model + c = \\approx y_p\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- thus such input features can be removed from a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9.1.1  Handeling missing feature information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real world data can contain *missing values* due to human error in collection, storage issues, faulty sensors, etc.,\n",
    "\n",
    "\n",
    "- If a supervised learning datapoint is missing its *output* value - e.g., if a classification datapoint is missing its *label* - there can be little we can do to salvage the datapoint, and usually throw it away\n",
    "\n",
    "\n",
    "- However a datapoint with missing *input (feature) values* can be salvaged\n",
    "\n",
    "\n",
    "- When data is a scarce resource, which can occur in buisness, statistics, and social science applications we do not want to just throw away data with missing *input* entries\n",
    "\n",
    "\n",
    "- If we have a datapoint $\\mathbf{x}_p$ with missing entries, we want our machine learning model to (naturally) *ignore* these missing entries since they cannot possible contribute to learning (they are missing after all)\n",
    "\n",
    "\n",
    "- However in order to *feed this datapoint* into a machine learning model we must set these missing entries to some numerical value(s), but what values should we set them too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Values closer to the mean of input feature values are *less* indicative than those further away\n",
    "\n",
    "\n",
    "- why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1  Handling categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume we want to classify individuals into two classes, affluent and not-affluent, using data that has, among many other features, the *most commonly used method of transportation* as an input feature with four possible outcomes: **walking**, **biking**, **driving**, and **public transportation**. How can we translate these outcomes into numbers decipherable by computers? Well, the first approach anyone might guess is to assign a distinct number to each outcome, e.g., 1 to walking, 2 to biking, 3 to driving, and 4 to public transportation. Seems easy enough!\n",
    "\n",
    "<img src=\"../../mlrefined_images/superlearn_images/dummy_1.png\" width=650 height=450/>\n",
    "\n",
    "There is however one issue with this approach. Imagine there are two individuals in our dataset, both belonging to the not-affluent class, who differ from each other only in terms of their most commonly used method of transportation. Lets call them Trey and Matt. Trey walks to work everyday while Matt takes the bus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we generally want that instances from the same class to stay close to each other in the feature space and far away from instances of the other class(es). The current encoding of the transportation feature does not satisfy this desire, at least not for Trey and Matt who are - with the current encoding - maximally distant from one another! But also remember we assigned numbers 1 through 4 to the four outcomes arbitrarily. We could have instead encoded the outcomes as follows\n",
    "\n",
    "<img src=\"../../mlrefined_images/superlearn_images/dummy_2.png\" width=650 height=450/>\n",
    "\n",
    "which, one could argue, better represents the data since *\"not-affluent individuals are more likely to walk or use public transportation and less likely to drive their own vehicle\"*. Regardless of how much this statement is true or can be trusted, one thing is clear: we need a better and more general way of encoding categorical features that does not rely on  our intuition or preconceived biases. \n",
    "\n",
    "Fortunately there is a simple way of fixing this issue. Instead of assigning a unique integer to each of the four outcomes, we can replace the transportation feature with four new 'dummy' features:\n",
    "\n",
    "* Is the most commonly used method of transportation, **walking**? 1 for yes, &nbsp; 0 for no\n",
    "\n",
    "* Is the most commonly used method of transportation, **biking**? &nbsp;&nbsp; 1 for yes, &nbsp; 0 for no\n",
    "\n",
    "* Is the most commonly used method of transportation, **driving**? &nbsp; 1 for yes, &nbsp; 0 for no\n",
    "\n",
    "* Is the most commonly used method of transportation, **public transportation**? &nbsp; 1 for yes,&nbsp; 0 for no\n",
    "\n",
    "This way the transportation feature will be encoded using these dummy features as a binary string of length four, with exactly one '1' and three '0's:  \n",
    "\n",
    "\n",
    "<img src=\"../../mlrefined_images/superlearn_images/Trey_Matt.png\" width=475 align=left height=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of encoding categorical features is sometimes referred to as *one-hot encoding*. Note that using this approach all possible outcomes will be equidistant from one another - at the cost of replacing the original feature with several dummy variables. Now that we know how to handle categorical features, lets use it to prepare a real dataset for classification.   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
