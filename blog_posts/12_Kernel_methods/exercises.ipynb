{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span>  Kernelizing the multiclass softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the multiclass softmax cost function is written as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b_{1},...,b_{C},\\mathbf{w}_{1},...,\\mathbf{w}_{C}\\right)=\\underset{c=1}{\\overset{C}{\\sum}}\\underset{p\\in\\Omega_{c}}{\\sum}\\mbox{log}\\left(1+\\underset{\\underset{j\\neq c}{j=1}}{\\overset{C}{\\sum}}e^{\\left(b_{j}^{\\,}-b_{c}^{\\,}\\right)+\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting each $\\mathbf{w}_{j}$ as $\\mathbf{w}_{j}=\\mathbf{F}\\mathbf{z}_{j}+\\mathbf{r}_{j}$\n",
    "where $\\mathbf{F}^{T}\\mathbf{r}_{j}=\\mathbf{0}_{P\\times1}$ for all\n",
    "$j$ we can rewrite each $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)$\n",
    "term as $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)=\\mathbf{f}_{p}^{T}\\left(\\mathbf{F}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)+\\left(\\mathbf{r}_{j}-\\mathbf{r}_{c}\\right)\\right)=\\mathbf{f}_{p}^{T}\\mathbf{F}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)$.\n",
    "And denoting $\\mathbf{H}=\\mathbf{F}^{T}\\mathbf{F}$ the kernel matrix\n",
    "this we have that $\\mathbf{f}_{p}^{T}\\left(\\mathbf{w}_{j}^{\\,}-\\mathbf{w}_{c}^{\\,}\\right)=\\mathbf{h}_{p}^{T}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)$, and so the cost may be written eqvuivalently (kernelized) as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(b_{1},...,b_{C},\\mathbf{z}_{1},...,\\mathbf{z}_{C}\\right)=\\underset{c=1}{\\overset{C}{\\sum}}\\underset{p\\in\\Omega_{c}}{\\sum}\\mbox{log}\\left(1+\\underset{\\underset{j\\neq c}{j=1}}{\\overset{C}{\\sum}}e^{\\left(b_{j}^{\\,}-b_{c}^{\\,}\\right)+\\mathbf{h}_{p}^{T}\\left(\\mathbf{z}_{j}-\\mathbf{z}_{c}\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "as shown in Table \\ref{tab:kernelized-versions}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span>  Prove the high capacity units grow combinatorially as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, if we chose a smaller set of say just those units with the same capacity - e.g., for polynomials this would be all units where $m_1 + m_2 + \\cdots + m_N = D$ - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 3. </span> Kernelizing the PCA Least Squares cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 10.?? we described the nonlinear extension of PCA via the PCA Least Squares cost introduced in Section 8.2 as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}_1,...,\\mathbf{w}_P,\\mathbf{C}\\right) = \\frac{1}{P}\\sum_{p = 1}^P \\left \\Vert \\mathbf{C}\\mathbf{w}_p - \\mathbf{f}_p \\right\\Vert_2^2.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose here that the feature transformed input $\\mathbf{f}_p$ of the input $\\mathbf{x}_p$ is precisely as defined in the prior Examples.  The classic solution to this cost is provided by the *eigenvector-eigenvalue decomposition* of the feature-transformed covariance matrix (see Section 8.4) $\\frac{1}{P}\\mathbf{F}\\mathbf{F}^{T} = \\mathbf{V}\\mathbf{D}\\mathbf{V}^T$.  However this is an $N\\times N$ matrix, and is *not* $P\\times P$ kernel matrix, which is given as $\\mathbf{H} = \\mathbf{F}^{T}\\mathbf{F}$.  While this simple fact indicates the impossibility of computing the eigenvectors of the *kernelized* feature tranformed input, we can compute the *encoded* version of the this kernelized input $\\mathbf{w}_p = \\mathbf{V}^T\\mathbf{f}_p$ directly via the kernel matrix as you will explore in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** As shown in Section 8.4, the eigenvectors of classic solution to the original PCA problem $\\frac{1}{P}\\mathbf{X}\\mathbf{X}^T = \\mathbf{V}\\mathbf{D}\\mathbf{V}^T$ may each be represented as some linear combination of the input points i.e., $\\mathbf{V} = \\mathbf{X}^T$.  \n",
    "\n",
    "\n",
    "are given as a linear combination of the input.  In the feature transformed case this statement still holds, and thus we ma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example ?. </span>  Kernelize K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
