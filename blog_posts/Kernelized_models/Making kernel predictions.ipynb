{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate new test points via a kernelized model we must - unlike any other nonlinearity - use **our entire training set**.  Denote the training set $\\left\\{\\mathbf{x}_p,\\,y_p\\right\\}_{p=1}^P$  \n",
    "\n",
    "Using the notation of the chapter I sent you, suppose we have used the polynomial kernel listed in example 7.1\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H}_{i,j} = \\left(1 + \\mathbf{x}_i^T\\mathbf{x}_j\\right)^D + 1.\n",
    "\\end{equation}\n",
    "\n",
    "When constructed for our training set this is a $P\\times P$ *symmetric* matrix whose $p^{th}$ column denoted by $\\mathbf{h}_p$\n",
    "is the $p^{th}$ training point evaluiated through the kernel, and so  involves *the entire training set* as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_p = \n",
    "\\begin{bmatrix}\n",
    "\\left(1 + \\mathbf{x}_1^T\\mathbf{x}_p\\right)^D + 1 \\\\\n",
    "\\left(1 + \\mathbf{x}_2^T\\mathbf{x}_p\\right)^D + 1 \\\\\n",
    "\\vdots \\\\\n",
    "\\left(1 + \\mathbf{x}_P^T\\mathbf{x}_p^{\\,}\\right)^D + 1\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, while these vectors involve the entire training set at least when training they are computed *only once* for the training data.  However this is not the case for test points - as we will see below. \n",
    "\n",
    "Notice then that to evaluate any other point $\\mathbf{x}$ through this kernel - like a test point we receive in the future - we must again employ the *entire training set*.  Our evaluation of the generic point $\\mathbf{x}$ through the kernel provides the same set of evaluations over the training set as the $p^{th}$ column of the original kernel matrix, only applied to the point $\\mathbf{x}$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = \n",
    "\\begin{bmatrix}\n",
    "\\left(1 + \\mathbf{x}_1^T\\mathbf{x}_{\\,}\\right)^D + 1 \\\\\n",
    "\\left(1 + \\mathbf{x}_2^T\\mathbf{x}_{\\,}\\right)^D + 1 \\\\\n",
    "\\vdots \\\\\n",
    "\\left(1 + \\mathbf{x}_P^T\\mathbf{x}_{\\,}\\right)^D + 1\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to our generic ``predict`` function - which is a (weighted) linear combination of a generic kerenlized input $\\mathbf{x}$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{predict}\\left(\\mathbf{x},\\omega\\right) = w_0 + \\mathbf{h}^T\\mathbf{z}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the way this is written our set $\\omega$ consists of the bias $w_0$ and the weight vector $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example if we are performing (nonlinear) regression *on the training set* we use this ``predict`` function and our Least Squares cost as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\omega\\right) = \\sum_{p=1}^P\\left(\\text{predict}\\left(\\mathbf{x}_p,\\omega\\right) - y_p \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "minimizing the above over $w_0$ and $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that we have properly minimized the cost function above and tuned our weights appropriately.  Denote $\\mathbf{x}_{\\text{test}}$ a new test point.  To evaluate it using the kernel we have the same formula as above, only now we evaluate the test point\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_{\\text{test}} = \n",
    "\\begin{bmatrix}\n",
    "\\left(1 + \\mathbf{x}_1^T\\mathbf{x}_{\\text{test}}^{\\,}\\right)^D + 1 \\\\\n",
    "\\left(1 + \\mathbf{x}_2^T\\mathbf{x}_{\\text{test}}^{\\,}\\right)^D + 1 \\\\\n",
    "\\vdots \\\\\n",
    "\\left(1 + \\mathbf{x}_P^T\\mathbf{x}_{\\text{test}}^{\\,}\\right)^D + 1\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes - to evaluate a new test point we must make use of the *entire training set* as well.  Our prediction for the new test point is given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{predict}\\left(\\mathbf{x}_{\\text{test}},\\omega\\right) = w_0 + \\mathbf{h}_{\\text{\\test}}^T\\mathbf{z}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
