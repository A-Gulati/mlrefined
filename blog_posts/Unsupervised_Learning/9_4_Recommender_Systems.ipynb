{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unsupervised Learning Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4  Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Section we discuss the fundamental linear *Recommender System*, a popular unsupervised learning framework commonly employed by businesses to help automatically recommend products and services to their customers.  From the vantage of machine learning however, the basic Recommender System detailed here is simply a slight twist on  our core unsupervised learning technique: Principal Component Analysis (PCA).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.1 Introduction and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems are heavily used in e-commerce today, providing customers with personalized recommendations for products and services by using a consumer's previous purchasing and rating history, along with those of similar customers.  For instance, a movie provider like Netflix with millions of users and tens of thousands of movies, records users' reviews and ratings (typically in the form of a number on a scale of 1-5 with 5 being the most favorable rating) in a large matrix such as the one illustrated below in the Figure. These matrices are very sparsely populated, since an individual consumer has likely rated only a small portion of the movies available. With this data available, online movie and commerce sites often use the unsupervised learning technique we discuss in this Section as their main technique for making personalized recommendations to customers regarding what they might like to watch / consume next. With the technique for producing personalized recommendations we discuss here - typically referred to as a *Recommender System* - we aim to intelligently guess the values of every missing entry in the ratings matrix (we *complete* the matrix).  Then, in order to recommend a new product to a given user, we examine our completely filled ratings matrix for products we have predicted the user would highly rate (and thus enjoy) and recommend these.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_11.png' width=\"100%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 1:</strong> <em> A prototypical movie rating matrix is very sparsely populated, with each user having rated only a very small number of films. In this diagram movies are listed along rows with users along columns.  In order to properly recommend movies for users to watch we try to intelligently guess the missing values of this matrix, and then recommend movies that we predict users would highly rate (and therefore enjoy the most).\n",
    "  </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# custom libs\n",
    "from mlrefined_libraries import unsupervised_library as unsuplib\n",
    "datapath = '../../mlrefined_datasets/unsuperlearn_datasets/'\n",
    "\n",
    "# this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.2  Formal modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will use the familiar notation $\\mathbf{x}_1,...,\\mathbf{x}_P$ to denote our input data, each of which has dimension $N$.   Here $\\mathbf{x}_p$ is our $p^{th}$ user's rating vector of - and is $N$ dimensional where $N$ is the total number of products / services available to be rated - and each is very sparsely populated (with whatever ratings user $p$ has input into the system).  Formally we can denote the index set of those products / services the $p^{th}$ user has rated - or in other words the index set of those non-empty entries of $\\mathbf{x}_p$ - as\n",
    "\n",
    "\\begin{equation}\n",
    "\\Omega_p = \\left \\{\\,\\,j\\,\\,\\rvert \\,\\,x_{j,\\,p}  \\,\\,\\text{exists}  \\right \\}.\n",
    "\\end{equation}\n",
    "\n",
    "Note here that $x_{j,\\,p}$ denotes the $j^{th}$ entry of the vector $\\mathbf{x}_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stacking these user-rating vectors columnwise gives us our ratings matrix that we wish to complete\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix} \n",
    "\\vert \\,\\,\\,\\,\\,\\, \\vert \\,\\,\\,\\,\\,...\\,\\,\\,\\,\\vert \\\\\n",
    "\\mathbf{x}_1 \\,\\, \\mathbf{x}_2 \\,\\,...\\,\\,\\mathbf{x}_P \\\\\n",
    "\\vert \\,\\,\\,\\,\\,\\, \\vert \\,\\,\\,\\,\\,...\\,\\,\\,\\,\\vert\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "In order to complete a sparsely populated ratings matrix effectively - because the data we are aiming to fill in is literally missing - we have no choice but to make assumptions about how users' tastes behave in general.   The most common / simplest assumption we can make - and the one we discuss here - is that every user's tastes can be expressed as a linear combination of some small set of fundamental user taste-profiles.  For example, in the case of movies these profiles could include the prototypical romance movie lover, prototypical comedy movie lover, action movie lover, etc.,  The relatively small number of such categories or user types compared to the total number of users or movies / products / etc., in a ratings matrix, provides a useful framework to intelligenty guess at the ratings matrix's missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this assumption help us complete the ratings matrix?  In order to find out we need to translate this intuitive assumption into mathematics.  When stated mathematically this assumption says that there are some set of $K < N$ prototypical user-rating profile basis vectors $\\mathbf{c}_1,\\,\\mathbf{c}_2,\\,..,\\,\\mathbf{c}_K$ so that we can express (approximately) every *complete* user-profile vector (given also the ideal weights in each linear combination) as \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{n=1}^K \\mathbf{c}_n w_{n,\\,p} \\approx \\mathbf{x}_p  \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, p=1...P\n",
    "\\end{equation}\n",
    "\n",
    "Again here we suppose that both the prototypical rating profile vectors $\\mathbf{c}_1,\\,...\\,\\mathbf{c}_K$ and the weights $w_{n,p}$ are ideal.  In practice we need to learn the proper values of these parameters, and from here we could then propose to square the difference of each desired approximation above and sum the result (which would give precisely the Least Squares cost function for PCA we derived in Section 9.2.1 and also 9.2.5).  But remember in the case of Recommender Systems we only have entries of $\\mathbf{X}$ belonging to the index sets $\\Omega_p$.  For each $p$ we will denote the desired approximation over only this index set as\n",
    "\n",
    "\\begin{equation}\n",
    "\\left.\\left\\{\\sum_{n=1}^K \\mathbf{c}_n w_{n,\\,p} \\approx \\mathbf{x}_p \\right\\}\\right\\vert_{\\,\\Omega_p}  \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, p=1...P.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring the difference between both sides of the above we can then form a Least Squares cost function for learning the parameters of our Recommneder System (that closely mirrors the one we derived for PCA, as written in Section 9.2.5) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}_1,...,\\mathbf{w}_P,\\mathbf{c}_1,...,\\mathbf{c}_K\\right) = \\frac{1}{P}\\sum_{p = 1}^P \\left \\Vert \\left.\\left\\{ \\sum_{n=1}^K\\mathbf{c}_nw_{n,\\,p} - \\mathbf{x}_p \\right\\}\\right\\vert_{\\,\\Omega_p}\\right\\Vert_2^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.3  Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how the setup and cost function of the linear Recommender Systems model mirrors PCA very closely.  Now we discuss how to minimize the Least Squares cost function above in order to recover optimal settings for our parameter vectors $\\mathbf{w}_1,...,\\mathbf{w}_p$ and $\\mathbf{c}_1,...,\\mathbf{c}_K$.  In the case of PCA, we saw in Section 9.2.2 how the additional assumption that our basis is *orthonormal* allowed us to derive mathematically simple forms for our ideal parameter vectors.  However here the additional wrinkle of the Recommender Systems model - that we only have access to data whose indices lie in the set $\\Omega$ - prohibits similar calculations.  Thus in short we must rely on an iterative method for learning ideal parameters here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in Section 9.2.5 we also saw how a simple *block-coordinate descent* approach, where we udpate one vector of parameters at a time, is a natural choice of algorithm to sequentially minimize the PCA cost function when the assumption of orthonormality is *not* enforced.  Because we now face the task of minimizing a highly related cost function for Recommender Systems it makes sense that we should apply the same approach, and so we will.  In other words, we will sequentially minimize the cost function above by sweeping through first the set of weights $\\mathbf{w}_1,...,\\mathbf{w}_P$ solving the first order system in each $\\mathbf{w}_p$ independently with all other parameters fixed, and then we do the same for $\\mathbf{c}_1,...,\\mathbf{c}_K$.  Working carefully through the equations one can formulate very similar updates to ones derived for PCA in Section 9.2.5.  We summarize the form of these update steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $\\mathbf{C}$ our $N\\times K$ basis matrix as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \\begin{bmatrix} \n",
    "\\vert \\,\\,\\,\\,\\,\\, \\vert \\,\\,\\,\\,\\,...\\,\\,\\,\\,\\vert \\\\\n",
    "\\mathbf{c}_1 \\,\\, \\mathbf{c}_2 \\,\\,...\\,\\,\\mathbf{c}_K \\\\\n",
    "\\vert \\,\\,\\,\\,\\,\\, \\vert \\,\\,\\,\\,\\,...\\,\\,\\,\\,\\vert\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and $\\mathbf{C}_{\\Omega_p}$ as the $\\left\\vert\\Omega_p\\right\\vert \\times K$ submatrix consisting of the $j^{th}$ row of $\\mathbf{C}$ for each $j \\in \\Omega_p$ the update step for $\\mathbf{w}_p$ can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}_{\\Omega_p}^T\\mathbf{C}_{\\Omega_p}^{\\,}\\mathbf{w}_p = \\mathbf{C}_{\\Omega_p}^T\\mathbf{x}_p \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, p=1...P .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how closely this resembles the analgous PCA update step given in equation (9.2.22).  Likewise the update step for $\\mathbf{c}_n$ we can write the update step as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{c}_n = \\frac{\\sum_{p\\, \\text{if} \\,n\\in \\Omega_p} \\mathbf{x}_p w_{n,\\,p} } {\\sum_{p\\, \\text{if} \\,n\\in \\Omega_p} w_{n,\\,p}^2}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, n=1...K\n",
    "\\end{equation}\n",
    "\n",
    "which mirrors the analagous update step for PCA given in equation (9.2.23). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweeping through the parameter vectors and performing these updates a number of times results in a proper solution to the Least Squares cost function detailed above.  Below we collect these update steps in a pseudo-code block for convenience.  Comparing this procedure to the block-coordinate descent algorithm described in Section 9.2.5, notice unsuprisingly that if the ratings matrix $\\mathbf{X}$ were *complete* these update steps would be precisely those given for the block-coordinate update steps for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender systems  (block-coordinate descent) \n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "<p style=\"line-height: 1.7;\">\n",
    "<strong>1:</strong>&nbsp;&nbsp; <strong>input:</strong> a number $K \\leq N$ of desired basis elements, dataset $\\mathbf{x}_1,...,\\mathbf{x}_P$, initializations for basis $\\mathbf{c}_1,...,\\mathbf{c}_K$, and maximum number of iterations $\\text{max_its}$ <br>\n",
    "\n",
    "<strong>2:</strong>&nbsp;&nbsp; <code>for</code> $\\,\\,i = 1,\\ldots,\\text{max_its}$<br>\n",
    "\n",
    "<strong>3:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code># Update weight vectors</code><br>\n",
    "\n",
    "<strong>4:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>for</code> $\\,\\,p = 1,\\ldots,P$<br>\n",
    "\n",
    "<strong>5:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code> solve </code> $\\mathbf{C}_{\\Omega_p}^T\\mathbf{C}_{\\Omega_p}^{\\,}\\mathbf{w}_p = \\mathbf{C}_{\\Omega_p}^T\\mathbf{x}_p$ <br>\n",
    "\n",
    "<strong>6:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>7:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code># Update basis</code><br>\n",
    "\n",
    "<strong>8:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>for</code> $\\,\\,n = 1,\\ldots,K$<br>\n",
    "\n",
    "<strong>9:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code> solve </code> $\\mathbf{c}_n = \\frac{\\sum_{p\\, \\text{if} \\,n\\in \\Omega_p} \\mathbf{x}_p w_{n,\\,p} } {\\sum_{p\\, \\text{if} \\,n\\in \\Omega_p} w_{n,\\,p}^2}$ <br>\n",
    "\n",
    "<strong>10:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>11:</strong>&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>12:</strong>&nbsp;&nbsp; <code># Update weights on final basis</code><br>\n",
    "\n",
    "\n",
    "<strong>13:</strong>&nbsp;&nbsp;&nbsp; <code>for</code> $\\,\\,p = 1,\\ldots,P$<br>\n",
    "\n",
    "<strong>14:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code> solve </code> $\\mathbf{C}_{\\Omega_p}^T\\mathbf{C}_{\\Omega_p}^{\\,}\\mathbf{w}_p = \\mathbf{C}_{\\Omega_p}^T\\mathbf{x}_p$ <br>\n",
    "\n",
    "<strong>15:</strong>&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>16:</strong>&nbsp; <strong>output:</strong> optimal Recommender System basis $\\mathbf{c}_1,...,\\mathbf{c}_K$ and weights $\\mathbf{w}_1,...,\\mathbf{w}_P$<br>\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1: </span>  A simple example learning a spanning set via Principal Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommender Systems Least Squares cost function\n",
    "def recommender_cost_function(X,C,W):\n",
    "    N,P = X.shape\n",
    "    K,P = W.shape\n",
    "    \n",
    "    # loop over points and construct cost value iteratively\n",
    "    cost = 0\n",
    "    A = np.dot(C,W)\n",
    "    for p in range(P):\n",
    "        # find indices of pth ratings vector that exist\n",
    "        ind = np.isnan(X[:,p])\n",
    "        ind = np.argwhere(ind == False).ravel()\n",
    "        \n",
    "        # compute least squares term over each basis element\n",
    "        x_p = X[:,p]\n",
    "        a_p = A[:,p]\n",
    "        \n",
    "        # add current basis cost to total cost \n",
    "        cost += np.linalg.norm(a_p[ind] - x_p[ind])**2\n",
    "        \n",
    "    return cost/float(P)\n",
    "\n",
    "def PCA_cost(X,C,W):\n",
    "    N,P = np.shape(X)\n",
    "    mask = np.isnan(X)\n",
    "    return  np.linalg.norm(np.dot(C,W) - X,'fro')**2/float(P)\n",
    "\n",
    "\n",
    "# function for updating weights - one column/element at a time\n",
    "def update_weights(X,C):\n",
    "    # Update weight vectors\n",
    "    N,P = X.shape\n",
    "    W = []\n",
    "    \n",
    "    # update each set of weights\n",
    "    for p in range(P):\n",
    "        # find indices of pth ratings vector that exist\n",
    "        ind = np.isnan(X[:,p])\n",
    "        ind = np.argwhere(ind == False).ravel()\n",
    "        \n",
    "        # pluck out appropriate subset of rows matching existing indicies\n",
    "        C_Om = C[ind,:]\n",
    "        CTC = np.dot(C[ind,:].T,C[ind,:])        \n",
    "\n",
    "        # setup linear system\n",
    "        w_p = np.array(np.linalg.lstsq(CTC,np.dot(C_Om.T,X[ind,p][:,np.newaxis]))).ravel()\n",
    "        W.append(w_p[0].ravel())\n",
    "        \n",
    "    # return weight matrix\n",
    "    W = np.array(W).T\n",
    "    return W\n",
    "\n",
    "# function for updating basis - one column/element at a time\n",
    "def update_basis(X,W):\n",
    "    # Update basis vectors\n",
    "    N,P = X.shape\n",
    "    K,P = W.shape\n",
    "    C = []\n",
    "    \n",
    "    # form sum over known entries of X\n",
    "    for n in range(K):\n",
    "        # loop over points, adding only existant entries \n",
    "        c_n = np.zeros((N,1))\n",
    "        d_n = 0\n",
    "        for p in range(P):\n",
    "            # find indices of pth ratings vector that exist\n",
    "            ind = np.isnan(X[:,p])\n",
    "            ind = np.argwhere(ind == False).ravel()\n",
    "            if np.size(ind) > 0:\n",
    "                # update numerator / denominator of update\n",
    "                c_n[ind] += X[ind,p][:,np.newaxis]*W[n,p]\n",
    "                d_n += W[n,p]**2\n",
    "            \n",
    "        # divide off denom\n",
    "        c_n /= d_n      \n",
    "        C.append(c_n.ravel())\n",
    "        \n",
    "    # return updated basis\n",
    "    C = np.array(C).T\n",
    "    return C\n",
    "\n",
    "# main function\n",
    "def Recommender_Systems(X,C,max_its):\n",
    "    # Outer loop - over iterations\n",
    "    for i in range(max_its):\n",
    "        # update weights\n",
    "        W = update_weights(X,C)\n",
    "        # print ('W updated')\n",
    "\n",
    "        # update basis\n",
    "        C = update_basis(X,W)\n",
    "        # print ('C updated')\n",
    "        print (i)\n",
    "        print (recommender_cost_function(X,C,W))\n",
    "        \n",
    "    # final weight update\n",
    "    W = update_weights(X,C)\n",
    "    \n",
    "    # return basis and weights\n",
    "    return C,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16.3793713276\n",
      "1\n",
      "15.035638983\n",
      "2\n",
      "14.9961746754\n",
      "3\n",
      "15.0034459345\n",
      "4\n",
      "14.9800657693\n",
      "5\n",
      "14.9858877756\n",
      "6\n",
      "14.9621106235\n",
      "7\n",
      "14.9683824751\n",
      "8\n",
      "14.9364999936\n",
      "9\n",
      "14.9451643053\n"
     ]
    }
   ],
   "source": [
    "### Test matrix ###\n",
    "# generate full ratings matrix\n",
    "P = 100; N = 5; K = 2;\n",
    "X_orig = np.ceil(5*np.random.rand(N,P))\n",
    "X = copy.deepcopy(X_orig)\n",
    "\n",
    "# remove percentage of entries\n",
    "removal_percentage = 0\n",
    "removal_portion = round(removal_percentage*np.size(X_orig))\n",
    "indices = np.random.permutation(np.size(X_orig))[:removal_portion]\n",
    "X.ravel()[indices] = np.nan\n",
    "\n",
    "# generate initialization for basis\n",
    "C = np.random.randn(N,K)\n",
    "\n",
    "# run recommender system\n",
    "max_its = 10\n",
    "C_final,W_final = Recommender_Systems(X,C,max_its=max_its)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = unsuplib.plot_utilities.Visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter.plot_cost_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
