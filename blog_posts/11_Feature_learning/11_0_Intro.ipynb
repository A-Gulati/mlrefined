{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 11: Principles of Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11.0  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 10 we saw how supervised and unsupervised learners alike can be extended to perform nonlinear learning via the use of arbitrary linear combination of nonlinear functions / feature transformations that we engineered ourselves by visually examining data.  For example, we expressed a general nonlinear model for regression and two-class classification as a weighted sum of $B$ nonlinear functions of our input as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\Theta\\right) = w_0 + f_1\\left(\\mathbf{x}\\right){w}_{1} +  f_2\\left(\\mathbf{x}\\right){w}_{2} + \\cdots + f_B\\left(\\mathbf{x}\\right)w_B\n",
    "\\end{equation}\n",
    "\n",
    "where $f_1,\\,f_2,\\,...\\,f_B$ are nonlinear parameterized or unparameterized functions / features and $w_0$ through $w_B$ (along with any additional weights internal to the nonlinear functions) are represented in the weight set $\\Theta$.  In the prior Chapter we also saw how similar generic models alow us to generalize a variety of supervised / unsupervised tasks to the nonlinear case. \n",
    "\n",
    "In this Chapter we detail the fundamental tools and principles of *feature learning* that allow us to choose form of a nonlinear model like this - including the particular form of the nonlinear transformations $f_1,\\,f_2,\\,...,f_B$, the number $B$ of them employed, as well as how the parameters of $\\Theta$ are tuned - *automatically for any dataset*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0.1  The limits of nonlinear feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to engineer nonlinear feature properly we cannot in general rely on visualizations - as most (particularly modern) datasets have far more than two inputs making visualization impossible.  Morever, even in cases where data visualization is possible, we cannot rely on our own pattern recognition skills either.  Take the two simple examples below - one (on the left) a regression dataset with $N=1$ dimensional input and the other (on the right) a two-class classification dataset with $N=2$ dimensional input.  Note that the true underlying nonlinear model used to generate the regression data is shown in dashed black, and likewise the true nonlinear decision boundary separating the two classes of the classification data is shown in dashed black as well.  We humans are typically taught only how to recognize the simplest of nonlinear patterns 'by eye' - like those created by elementary functions (e.g., low degree polynomials, exponential functions, sine waves) and simple shapes (e.g., circles, squares, etc.,).   Neither of the patterns shown here match such simple nonlinear functionality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src= '../../mlrefined_images/nonlinear_superlearn_images/nonlinear_combined.png' width=\"75%\" height=\"75%\" alt=\"\"/>\n",
    "<figcaption>   \n",
    "<strong>Figure 1:</strong> <em> \n",
    "A regression (on the left - with true nonlinear model shown in dashed black) and two-class classification (on the right - with true nonlinear decision boundary shown in dashed black) datasets that clearly exhibit nonlinear behavior.  However in each case this behavior is difficult to assess 'by eye', since neither belongs to the basic set of nonlinear patterns we humans are taught to recognize.  Therefore even in cases like these - where we can visualize data - it can be difficult if not impossible to properly engineer nonlinear features ourselves.\n",
    "</em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So whether or not a dataset can be visualized, as the two examples above illustrate, engineering proper nonlinear features ourselves can be difficult if not impossible to do ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is precisely this challenge which motivates the fundamental *feature learning* tools described in this Chapter.  *In short these technologies *automate* the process of identifying appropriate nonlinear features for arbitrary datasets.*  With these tools in hand we no longer need to 'engineer' proper nonlinearites - at least in terms of how we 'engineered' nonlinear features in the previous Chapter - we instead aim at *learning* their appropriate forms (hence the phrase *feature learning*).  Compared to our own limited nonlinear pattern recognition abilities, feature learning tools can identify virtually any nonlinear pattern present in a dataset regardless of its input dimension.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0.2  Chapter summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim to automate nonnlinear learning is an ambitious one and is perhaps at first glance an intimidating one as well, for there are an infinite variety of nonlinearities / nonlinear functions to choose from.  How do we, in general, parse this infinitude automatically to determine the appropriate nonlinearity for a given dataset?  \n",
    "\n",
    "The first step - as we will see in Section 10.1 - is to organize the pursuit of automation by first placing the fundamental building blocks of this infinitude into *managable collections* of (relatively simple) nonlinear functions.  These collections are often called *universal approximators*, of which three strains are popularly used and which we introduce here: kernels, neural networks, and trees.  After introducing universal approximators we then discuss the fundamental concepts underlying how they are employed automatically - including a description of the *bias-variance tradeoff* in Section 11.2, the necessity for *validation error* as a measurement tool in Section 11.3, the automatic tuning of nonlinear capacity via *boostingn and regularization* in Sections 11.4 and 11.5 respectively, and the notion *ensembling* in Section 11.6, and finally *testing error* in Section 11.7.\n",
    "\n",
    "There is - as we will see - still considerable engineering effort required to perform feature learning properly.  However this engineering effort is of a completely different nature than the highly 'visualization-based' engineering effort detailed in the previous Chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; This material is not to be distributed, copied, or reused without written permission from the authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
