{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autograd functionalities\n",
    "import autograd.numpy as np\n",
    "from autograd import grad as compute_grad   \n",
    "\n",
    "# import plotting library and other necessities\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# import general libraries\n",
    "import copy\n",
    "from datetime import datetime \n",
    "\n",
    "#this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test transformation on face images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data, transform via original method, transform via new method, compare features to make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "datapath = '../../mlrefined_datasets/convnet_datasets/feat_face_data.csv'\n",
    "data = np.loadtxt(datapath,delimiter = ',')\n",
    "\n",
    "# import data and reshape appropriately\n",
    "x = data[:,:-1]\n",
    "y = data[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just examine a subset of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random subset of full dataset\n",
    "ind = np.random.permutation(len(y))\n",
    "x = x[ind[:1000],:]\n",
    "y = y[ind[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor-based version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tensor_conv_layer:    \n",
    "    # convolution function\n",
    "    def conv_function(self,tensor_window):\n",
    "        tensor_window = np.reshape(tensor_window,(np.shape(tensor_window)[0],np.shape(tensor_window)[1]*np.shape(tensor_window)[2]))\n",
    "        t = np.dot(self.kernels,tensor_window.T)\n",
    "        return t\n",
    "\n",
    "    # pooling / downsampling parameters\n",
    "    def pool_function(self,tensor_window):\n",
    "        t = np.max(tensor_window,axis = (1,2))\n",
    "        return t\n",
    "\n",
    "    # activation \n",
    "    def activation(self,tensor_window):\n",
    "        return np.maximum(0,tensor_window)\n",
    "\n",
    "    # pad image with appropriate number of zeros for convolution\n",
    "    def pad_tensor(self,tensor,kernel_size):\n",
    "        odd_nums = np.array([int(2*n + 1) for n in range(100)])\n",
    "        pad_val = np.argwhere(odd_nums == kernel_size)[0][0]\n",
    "        tensor_padded = np.zeros((np.shape(tensor)[0], np.shape(tensor)[1] + 2*pad_val,np.shape(tensor)[2] + 2*pad_val))\n",
    "        tensor_padded[:,pad_val:-pad_val,pad_val:-pad_val] = tensor\n",
    "        return tensor_padded    \n",
    "    \n",
    "    # sliding window for image augmentation\n",
    "    def sliding_window_tensor(self,tensor,window_size,stride,func):\n",
    "        # grab image size, set container for results\n",
    "        image_size = np.shape(tensor)[1]\n",
    "        results = []\n",
    "        \n",
    "        # slide window over input image with given window size / stride and function\n",
    "        for i in np.arange(0, image_size - window_size + 1, stride):\n",
    "            for j in np.arange(0, image_size - window_size + 1, stride):\n",
    "                # take a window of input tensor\n",
    "                tensor_window =  tensor[:,i:i+window_size, j:j+window_size]\n",
    "                \n",
    "                # now process entire windowed tensor at once\n",
    "                tensor_window = np.array(tensor_window)\n",
    "                yo = func(tensor_window)\n",
    "\n",
    "                # store weight\n",
    "                results.append(yo)\n",
    "        \n",
    "        # re-shape properly\n",
    "        results = np.array(results)\n",
    "        results = results.swapaxes(0,1)\n",
    "        if func == self.conv_function:\n",
    "            results = results.swapaxes(1,2)\n",
    "        return results \n",
    "\n",
    "    # make feature map\n",
    "    def make_feature_tensor(self,tensor):\n",
    "        # create feature map via convolution --> returns flattened convolution calculations\n",
    "        conv_stride = 1\n",
    "        feature_tensor = self.sliding_window_tensor(tensor,self.kernel_size,conv_stride,self.conv_function) \n",
    "\n",
    "        # re-shape convolution output ---> to square of same size as original input\n",
    "        num_filters = np.shape(feature_tensor)[0]\n",
    "        num_images = np.shape(feature_tensor)[1]\n",
    "        square_dim = int((np.shape(feature_tensor)[2])**(0.5))\n",
    "        feature_tensor = np.reshape(feature_tensor,(num_filters,num_images,square_dim,square_dim))\n",
    "        \n",
    "        # shove feature map through nonlinearity\n",
    "        feature_tensor = self.activation(feature_tensor)\n",
    "\n",
    "        # pool feature map --- i.e., downsample it\n",
    "        pool_stride = 3\n",
    "        pool_window_size = 6\n",
    "        downsampled_feature_map = []\n",
    "        for t in range(np.shape(feature_tensor)[0]):\n",
    "            temp_tens = feature_tensor[t,:,:,:]\n",
    "            d = self.sliding_window_tensor(temp_tens,pool_window_size,pool_stride,self.pool_function)\n",
    "            downsampled_feature_map.append(d)\n",
    "        downsampled_feature_map = np.array(downsampled_feature_map)\n",
    "\n",
    "        # return downsampled feature map --> flattened\n",
    "        return downsampled_feature_map\n",
    "\n",
    "    # our normalization function\n",
    "    def normalize(self,data,data_mean,data_std):\n",
    "        normalized_data = (data - data_mean)/(data_std + 10**(-5))\n",
    "        return normalized_data\n",
    "\n",
    "    # convolution layer\n",
    "    def conv_layer(self,tensor,kernels):\n",
    "        #### prep input tensor #####\n",
    "        # pluck out dimensions for image-tensor reshape\n",
    "        num_images = np.shape(tensor)[0]\n",
    "        num_kernels = np.shape(kernels)[0]\n",
    "        \n",
    "        # create tensor out of input images (assumed to be stacked vertically as columns)\n",
    "        tensor = np.reshape(tensor,(np.shape(tensor)[0],int((np.shape(tensor)[1])**(0.5)),int( (np.shape(tensor)[1])**(0.5))),order = 'F')\n",
    "\n",
    "        # pad tensor\n",
    "        kernel = kernels[0]\n",
    "        self.kernel_size = np.shape(kernel)[0]\n",
    "        padded_tensor = self.pad_tensor(tensor,self.kernel_size)\n",
    "\n",
    "        #### prep kernels - reshape into array for more effecient computation ####\n",
    "        self.kernels = np.reshape(kernels,(np.shape(kernels)[0],np.shape(kernels)[1]*np.shape(kernels)[2]))\n",
    "        \n",
    "        #### compute convolution feature maps / downsample via pooling one map at a time over entire tensor #####\n",
    "        # compute feature map for current image using current convolution kernel\n",
    "        feature_tensor = self.make_feature_tensor(padded_tensor)\n",
    "\n",
    "        feature_tensor = feature_tensor.swapaxes(0,1)\n",
    "        feature_tensor = np.reshape(feature_tensor, (np.shape(feature_tensor)[0],np.shape(feature_tensor)[1]*np.shape(feature_tensor)[2]),order = 'F')\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    ##### some supervised learning capabilities #####\n",
    "    def load_data(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self,x,w):\n",
    "        # pass input data through convolutional layer\n",
    "        x_conv = self.conv_layer(x,w[0])\n",
    "        \n",
    "        # take inner product against output of conv layer\n",
    "        value = w[1][0] + np.dot(x_conv,w[1][1:])\n",
    "        return value\n",
    "    \n",
    "    # the softmax cost function \n",
    "    def softmax(self,w):\n",
    "        cost  = np.sum(np.log(1 + np.exp((-self.y)*(self.predict(self.x,w)))))\n",
    "        return cost\n",
    "    \n",
    "    def count(self,w):\n",
    "        return 0.25*np.sum((np.sign(self.predict(self.x,w)) - self.y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd.misc.flatten import flatten\n",
    "from autograd.misc.flatten import flatten_func\n",
    "\n",
    "# gradient descent function\n",
    "def gradient_descent(g,w_unflat,alpha,max_its,version,**kwargs):\n",
    "    verbose = False\n",
    "    if 'verbose' in kwargs:\n",
    "        verbose = kwargs['verbose']\n",
    "\n",
    "    # flatten the input function, create gradient based on flat function\n",
    "    g_flat, unflatten, w = flatten_func(g, w_unflat)\n",
    "    grad = compute_grad(g)\n",
    "\n",
    "    # record history\n",
    "    w_hist = []\n",
    "    w_hist.append(w_unflat)\n",
    "\n",
    "    # over the line\n",
    "    for k in range(max_its):   \n",
    "        if verbose == True:\n",
    "            if np.mod(k,5) == 0:\n",
    "                print ('started step ' + str(k) + ' of ' + str(max_its))\n",
    "                \n",
    "        # plug in value into func and derivative\n",
    "        grad_eval = grad(w_unflat)\n",
    "        grad_eval, _ = flatten(grad_eval)\n",
    "\n",
    "        ### normalized or unnormalized descent step? ###\n",
    "        if version == 'normalized':\n",
    "            grad_norm = np.linalg.norm(grad_eval)\n",
    "            if grad_norm == 0:\n",
    "                grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n",
    "            grad_eval /= grad_norm\n",
    "\n",
    "        # take descent step \n",
    "        w = w - alpha*grad_eval\n",
    "\n",
    "        # record weight update\n",
    "        w_unflat = unflatten(w)\n",
    "        w_hist.append(w_unflat)\n",
    "        \n",
    "    if verbose == True:\n",
    "        print ('finished all ' + str(max_its) + ' steps')\n",
    "#     if verbose == True:\n",
    "#         print ('...optimization complete!')\n",
    "#         time.sleep(1.5)\n",
    "#         clear_output()\n",
    "\n",
    "    return w_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random initialization for kernels\n",
    "scale = 0.1\n",
    "kernels = scale*np.random.randn(8,3,3) \n",
    "\n",
    "# initialize instance of convnet\n",
    "tensor_test = tensor_conv_layer()\n",
    "tensor_test.load_data(x,y)\n",
    "\n",
    "# run a small set of images through the network to collect final weights size\n",
    "final_features = tensor_test.conv_layer(x,kernels)\n",
    "final_size = np.shape(final_features)[1] + 1\n",
    "\n",
    "# set final weights \n",
    "final_weights = scale*np.random.randn(final_size,1)\n",
    "weights = [kernels,final_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started step 0 of 50\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now() \n",
    "\n",
    "# run standard (full) gradient descent\n",
    "g = tensor_test.softmax\n",
    "w = weights\n",
    "alpha = 10**(-1)\n",
    "max_its = 50\n",
    "version = 'normalized'\n",
    "weight_history_1 =  gradient_descent(g,w,alpha,max_its,version,verbose = True)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime \n",
    "\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our plotting function\n",
    "def plot_history(x,y,weight_histories,soft,count):\n",
    "    '''\n",
    "    A module for computing / plotting the cost and misclassification histories for a given run of gradient descent.\n",
    "    Here the input should be the data and weight history from a gradient descent run\n",
    "    '''\n",
    "    \n",
    "    # initialize figure\n",
    "    fig = plt.figure(figsize = (9,3))\n",
    "\n",
    "    # create subplot with 3 panels, plot input function in center plot\n",
    "    gs = gridspec.GridSpec(1, 2) \n",
    "    ax1 = plt.subplot(gs[0]); \n",
    "    ax2 = plt.subplot(gs[1]);\n",
    "    \n",
    "    # loop over histories and plot all\n",
    "    c = 1\n",
    "    for weight_history in weight_histories:\n",
    "        # loop over input weight history and create associated cost and misclassification histories\n",
    "        cost_history = []\n",
    "        count_history = []\n",
    "        for weight in weight_history:\n",
    "            cost_val = soft(weight)\n",
    "            cost_history.append(cost_val)\n",
    "\n",
    "            count_val = count(weight)\n",
    "            count_history.append(count_val)\n",
    "\n",
    "        # now plot each, one per panel\n",
    "        ax1.plot(cost_history)  \n",
    "        label = 'full grad'\n",
    "        if c == 2:\n",
    "            label = 'mini-batch'\n",
    "        if c == 3:\n",
    "            label = 'stochastic'\n",
    "        ax2.plot(count_history,label = label)\n",
    "        c+=1\n",
    "        \n",
    "    # label each panel\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('cost function val')\n",
    "    ax1.set_title('cost function history')\n",
    "    \n",
    "    ax2.set_xlabel('iteration')\n",
    "    ax2.set_ylabel('misclassifications')\n",
    "    ax2.set_title('number of misclassificaions')\n",
    "    \n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_histories = [weight_history_1]\n",
    "plot_history(x,y,weight_histories,tensor_test.softmax,tensor_test.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
