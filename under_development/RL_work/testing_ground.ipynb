{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named large_gridworld_ipythonblocks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b05793d1f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlarge_gridworld_ipythonblocks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlockGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named large_gridworld_ipythonblocks"
     ]
    }
   ],
   "source": [
    "from large_gridworld_ipythonblocks import BlockGrid\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class my_gridworld():\n",
    "    \n",
    "    def __init__(self,grid_size):\n",
    "        # use either a small pre-staged grid world or big one with random hazards\n",
    "        \n",
    "        ### initialize grid, agent, obstacles, etc.,\n",
    "        self.width = 5\n",
    "        self.height = 4\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "        \n",
    "        # decide on hazards and goal locations\n",
    "        self.hazards = [[0,3],[1,3],[2,3]]  # impenetrable obstacle locations          \n",
    "        self.goal = [0,4]     # goal block\n",
    "        self.player = [0,0]   # initial location player\n",
    "        \n",
    "        # index states for Q matrix\n",
    "        self.states = []\n",
    "        for i in range(self.grid.height):\n",
    "            for j in range(self.grid.width):\n",
    "                block = [i,j]\n",
    "                self.states.append(str(i) + str(j))\n",
    "        \n",
    "        if grid_size == 'large':\n",
    "            ### initialize grid, agent, obstacles, etc.,\n",
    "            self.width = 20\n",
    "            self.height = 10\n",
    "            self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "            # decide on player and goal locations\n",
    "            self.goal = [0,self.width-1]     # goal block\n",
    "            self.player = [0,0]   # initial location player\n",
    "            \n",
    "            # index states for Q matrix\n",
    "            self.states = []\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    block = [i,j]\n",
    "                    self.states.append(str(i) + str(j))\n",
    "\n",
    "            # decide on hazard locations\n",
    "            num_hazards = 50\n",
    "            self.hazards = []\n",
    "            inds = np.random.permutation(self.grid.width*self.grid.height)\n",
    "            inds = inds[:num_hazards]\n",
    "            k = 0\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    if k in inds: \n",
    "                        block = [i,j]\n",
    "                        if block != self.goal:\n",
    "                            self.hazards.append(block)\n",
    "                    k+=1\n",
    "                                  \n",
    "        if grid_size[0:4] == 'maze':\n",
    "            \n",
    "            df = pd.read_csv('maze_small.csv',header = None)\n",
    "            ### initialize grid, agent, obstacles, etc.,            \n",
    "            self.width = 13\n",
    "            self.height = 11\n",
    "       \n",
    "            if grid_size == 'maze_large':\n",
    "                df = pd.read_csv('maze_large.csv',header = None)\n",
    "                ### initialize grid, agent, obstacles, etc.,            \n",
    "                self.width = 41\n",
    "                self.height = 15\n",
    "\n",
    "            self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "            # decide on player and goal locations\n",
    "            self.goal = [self.height-2, self.width-1]     # goal block\n",
    "            self.player = [self.height-2, 0]   # initial location player\n",
    "            \n",
    "            # index states for Q matrix\n",
    "            self.states = []\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    block = [i,j]\n",
    "                    self.states.append(str(i) + str(j))\n",
    "\n",
    "            # decide on hazard locations\n",
    "            self.hazards = []\n",
    "            inds = df.values[0]\n",
    "            k = 0\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    if k in inds: \n",
    "                        block = [i,j]\n",
    "                        if block != self.goal:\n",
    "                            self.hazards.append(block)\n",
    "                    k+=1   \n",
    "                        \n",
    "        # initialize action choices\n",
    "        self.action_choices = [[-1,0],[1,0],[0,-1],[0,1]]\n",
    "        \n",
    "        # initialize Q^* matrix\n",
    "        self.Q_star = np.zeros((self.grid.width*self.grid.height,len(self.action_choices)))\n",
    "        \n",
    "    def color_grid(self):                            \n",
    "        # remake + recolor grid\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "        # color obstacles\n",
    "        for i in range(len(self.hazards)):\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].green = 100\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].red = 250\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].blue = 0\n",
    "\n",
    "        # make and color goal\n",
    "        self.grid[self.goal[0],self.goal[1]].green = 255\n",
    "        self.grid[self.goal[0],self.goal[1]].red = 0\n",
    "        self.grid[self.goal[0],self.goal[1]].blue = 0\n",
    "        \n",
    "        # color player location\n",
    "        self.grid[self.player[0],self.player[1]].green = 0\n",
    "        self.grid[self.player[0],self.player[1]].red = 0\n",
    "        self.grid[self.player[0],self.player[1]].blue = 200\n",
    "        \n",
    "        self.grid.show()\n",
    "        \n",
    "    ## Q-learning function\n",
    "    def qlearn(self,gamma,hazard_penalty,num_train_animate):\n",
    "        num_episodes = 300\n",
    "        num_complete = 0\n",
    "        \n",
    "        # loop over episodes, for each run simulation and update Q\n",
    "        for n in range(num_episodes):\n",
    "            # pick random initialization \n",
    "            obstical_free = 0\n",
    "            loc = [np.random.randint(self.grid.height),np.random.randint(self.grid.width)]\n",
    "           \n",
    "            # update Q matrix while loc != goal\n",
    "            steps = 0  # step counter\n",
    "            max_steps = 10*self.grid.width*self.grid.height  # maximum number of steps per episode\n",
    "            while steps < max_steps:    \n",
    "                # when you reach the goal end current episode\n",
    "                if loc == self.goal:\n",
    "                    break\n",
    "                    \n",
    "                ### choose action - left = 0, right = 1, up = 2, down = 3\n",
    "                k = np.random.randint(len(self.action_choices))  \n",
    "                loc2 = [sum(x) for x in zip(loc, self.action_choices[k])] \n",
    "                ind_old = self.states.index(str(loc[0]) + str(loc[1]))\n",
    "\n",
    "                ### set reward    \n",
    "                # is the new location just a regular square?  Than small negative reward\n",
    "                r_k = int(-1)\n",
    "\n",
    "                # if new state is hazard penalize with medium negative value - this needs to be set properly if you're trying to prove a point i.e., that a trained agent won't walk over one of these unless going around costs more\n",
    "                if loc2 in self.hazards:\n",
    "                    r_k = int(hazard_penalty)\n",
    "\n",
    "                # if new state is goal set reward of 0\n",
    "                if loc2 == self.goal:\n",
    "                    r_k = int(0)\n",
    "                    \n",
    "                # if new state is outside of boundaries of grid world penalize set reward to small negative value (like -1) and do not move\n",
    "                if loc2[0] > self.grid.height-1 or loc2[0] < 0 or loc2[1] > self.grid.width-1 or loc2[1] < 0:  \n",
    "                    r_k = int(-1)\n",
    "                    loc2 = loc\n",
    "                \n",
    "                ### Update Q\n",
    "                ind_new = self.states.index(str(loc2[0]) + str(loc2[1]))\n",
    "                self.Q_star[ind_old,k] = r_k + gamma*max(self.Q_star[ind_new,:])\n",
    "                    \n",
    "                # update current location of player to one we just moved too (or stay still if grid world boundary met)\n",
    "                self.player = loc2\n",
    "                loc = loc2\n",
    "                \n",
    "                # the next few lines just animate the first few steps during the first few episodes\n",
    "                if n < num_train_animate and steps < 200:\n",
    "                    self.color_grid()\n",
    "                    time.sleep(0.1)\n",
    "                    display.clear_output(wait=True)\n",
    "                    \n",
    "                # update counter\n",
    "                steps+=1\n",
    "        \n",
    "            # pause briefly between first few episodes to show user cutoff\n",
    "            if n < num_train_animate:\n",
    "                time.sleep(1)\n",
    "                print 'finished episode, animating next episode'\n",
    "                time.sleep(1)\n",
    "            if n == num_train_animate:\n",
    "                print 'continuing with remaining episodes without animation...'\n",
    "                \n",
    "        print 'q-learning process complete'\n",
    "                \n",
    "    # print out\n",
    "    def show_qmat(self):        \n",
    "        states_print = []\n",
    "        for i in range(len(self.states)):\n",
    "            s = str(self.states[i])\n",
    "            t = str('(') + s[0] + ',' + s[1] + str(')')\n",
    "            states_print.append(t)\n",
    "            \n",
    "        df = pd.DataFrame(self.Q_star,columns=['up','down','left','right'], index=states_print)\n",
    "        print df.round(3)             \n",
    "            \n",
    "    # animate the testing phase based on completed Q-learning cycle\n",
    "    def animate_testing_phase(self,loc):\n",
    "        # show movement based on an initial \n",
    "        self.player = loc # initial agent location\n",
    "\n",
    "        # if you chose an invalid starting position, break out and try again\n",
    "        if  loc == self.goal or loc[0] > self.grid.height-1 or loc[0] < 0 or loc[1] > self.grid.width-1 or loc[1] < 0:\n",
    "            print 'initialization is outside of gridworld or is goal, try again'\n",
    "        else:  \n",
    "            # now use the learned Q* matrix to run from any (valid) initial point to the goal\n",
    "            self.color_grid()\n",
    "            time.sleep(0.3)\n",
    "            display.clear_output(wait=True)\n",
    "            count = 0\n",
    "            max_count = self.grid.width*self.grid.height\n",
    "            while count < max_count:\n",
    "                # find next state using max Q* value\n",
    "                ind_old = self.states.index(str(self.player[0]) + str(self.player[1]))\n",
    "                \n",
    "                # find biggest value in Q* and determine block location\n",
    "                action_ind = np.argmax(self.Q_star[ind_old,:])\n",
    "                action = self.action_choices[action_ind]\n",
    "                \n",
    "                # move player to new location and recolor - if you move out of gridworld halt and report this to user\n",
    "                new_loc = [sum(x) for x in zip(self.player,action)] \n",
    "                \n",
    "                if new_loc[0] > self.grid.height-1 or new_loc[0] < 0 or new_loc[1] > self.grid.width-1 or new_loc[1] < 0:\n",
    "                    print 'something went wrong - the player has left the gridworld arena'\n",
    "                    print \"your episodes did not the states reached by your initialization enough to train Q properly here\"\n",
    "                    print \"this is likely because you didn't traiin long enough - up the number of steps / episodes and try again\"\n",
    "                    \n",
    "                self.player = new_loc\n",
    "                \n",
    "                # clear current screen for next step\n",
    "                self.color_grid()\n",
    "                time.sleep(0.2)\n",
    "                if self.player == self.goal:\n",
    "                    break\n",
    "                display.clear_output(wait=True)\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
