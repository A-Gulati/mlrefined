{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.animation as animation\n",
    "from JSAnimation import IPython_display\n",
    "import copy\n",
    "\n",
    "\n",
    "class my_gridworld_2():\n",
    "    \n",
    "    def __init__(self,**args):\n",
    "                \n",
    "        ### initialize global containers and variables\n",
    "        # initialize containers for grid, hazard locations, agent and goal locations, etc.,\n",
    "        self.grid = []\n",
    "        self.hazards = []\n",
    "        self.agent = []\n",
    "        self.goal = []\n",
    "        self.training_episodes_history = []\n",
    "\n",
    "        # initialize global variables e.g., height and width of gridworld, hazard penalty value\n",
    "        self.width = 0\n",
    "        self.height = 0\n",
    "        self.hazard_reward = 0\n",
    "        self.goal_reward = 0\n",
    "        self.standard_reward = -1\n",
    "        \n",
    "        ### check for optional arguments controlling size of gridworld, number/placement/value of hazards.  If no args given go to default.\n",
    "        # check if a pre-defined gridworld is desired\n",
    "        if \"world\" in args:\n",
    "            # small demo gridworld\n",
    "            if args['world'] == 'small demo':\n",
    "                ### initialize grid, agent, obstacles, etc.,\n",
    "                self.width = 5\n",
    "                self.height = 4\n",
    "                self.grid = np.zeros((self.height,self.width))\n",
    "\n",
    "                # initialize goal location\n",
    "                self.goal = [0,self.width-1]     # goal block\n",
    "                \n",
    "                # initialize agent location\n",
    "                self.agent = [0,0]   # initial location agent\n",
    "                \n",
    "                # initialize hazard locations\n",
    "                self.hazards = [[0,3],[1,3],[2,3]]  # impenetrable obstacle locations          \n",
    "                for i in range(len(self.hazards)): \n",
    "                    block = self.hazards[i]\n",
    "                    self.hazards.append(block)\n",
    "                    self.grid[block[0]][block[1]] = 1\n",
    "                                                  \n",
    "            # small random gridworld\n",
    "            if args['world'] == 'small random':\n",
    "                ### initialize locations ###\n",
    "                # initialize grid, agent, hazards, etc.,\n",
    "                self.width = 5\n",
    "                self.height = 5\n",
    "                self.grid = np.zeros((self.height,self.width))\n",
    "                            \n",
    "                # initialize goal location\n",
    "                self.goal = [0,self.width-1]     # goal block\n",
    "                \n",
    "                # initialize agent location\n",
    "                self.agent = [0,0]   # initial location agent\n",
    "                \n",
    "                # initialize random hazards locations\n",
    "                num_hazards = 15\n",
    "                self.hazards = []\n",
    "                inds = np.random.permutation(self.width*self.height)\n",
    "                inds = inds[:num_hazards]\n",
    "                k = 0\n",
    "                for i in range(self.height):\n",
    "                    for j in range(self.width):\n",
    "                        if k in inds: \n",
    "                            block = [i,j]\n",
    "                            if block != self.goal and block != self.agent:\n",
    "                                self.hazards.append(block)\n",
    "                                self.grid[block[0]][block[1]] = 1\n",
    "                        k+=1\n",
    "            \n",
    "            # initialize a big gridworld with randomly placed hazards.  \n",
    "            if args[\"world\"] == 'big random':\n",
    "                ### initialize locations ###\n",
    "                # initialize grid, agent, hazards, etc.,\n",
    "                self.width = 20\n",
    "                self.height = 10\n",
    "                self.grid = np.zeros((self.height,self.width))\n",
    "\n",
    "                # initialize goal location\n",
    "                self.goal = [0,self.width-1]     # goal block\n",
    "                \n",
    "                # initialize agent location\n",
    "                self.agent = [0,0]   # initial location agent\n",
    "                \n",
    "                # initialize random hazards locations\n",
    "                num_hazards = 50\n",
    "                self.hazards = []\n",
    "                inds = np.random.permutation(self.width*self.height)\n",
    "                inds = inds[:num_hazards]\n",
    "                k = 0\n",
    "                for i in range(self.height):\n",
    "                    for j in range(self.width):\n",
    "                        if k in inds: \n",
    "                            block = [i,j]\n",
    "                            if block != self.goal and block != self.agent:\n",
    "                                self.hazards.append(block)\n",
    "                                self.grid[block[0]][block[1]] = 1\n",
    "                        k+=1\n",
    "        \n",
    "            # small maze gridworld                      \n",
    "            if args[\"world\"] == 'small maze':\n",
    "                # load in preset hazard locations from csv\n",
    "                hazards = pd.read_csv('RL_datasets/small_maze.csv',header = None)\n",
    "                \n",
    "                ### initialize grid, agent, obstacles, etc.,            \n",
    "                self.width = 13\n",
    "                self.height = 11\n",
    "                self.grid = np.zeros((self.height,self.width))\n",
    "                \n",
    "                # initialize goal location\n",
    "                self.goal = [self.height-2, self.width-1]     # goal block\n",
    "                \n",
    "                # initialize agent location\n",
    "                self.agent = [self.height-2, 0]   # initial location agent\n",
    "  \n",
    "                # initialize hazards locations\n",
    "                for i in range(len(hazards)):\n",
    "                    block = list(hazards.iloc[i])\n",
    "                    self.hazards.append(block)\n",
    "                    self.grid[block[0]][block[1]] = 1\n",
    "        \n",
    "            # big maze gridworld                      \n",
    "            if args[\"world\"] == 'big maze':\n",
    "                # load in preset hazard locations from csv\n",
    "                hazards = pd.read_csv('RL_datasets/big_maze.csv',header = None)\n",
    "                \n",
    "                ### initialize grid, agent, obstacles, etc.,            \n",
    "                self.width = 41\n",
    "                self.height = 15\n",
    "                self.grid = np.zeros((self.height,self.width))\n",
    "\n",
    "                # initialize goal location\n",
    "                self.goal = [self.height-2, self.width-1]     # goal block\n",
    "                self.grid[self.goal[0]][self.goal[1]] = 2\n",
    "\n",
    "                # initialize agent location\n",
    "                self.agent = [self.height-2, 0]   # initial location agent\n",
    "                \n",
    "                # initialize hazards locations\n",
    "                for i in range(len(hazards)):\n",
    "                    block = list(hazards.iloc[i])\n",
    "                    self.grid[block[0]][block[1]] = 1\n",
    "                    \n",
    "        ### check for hazard penalty value ###\n",
    "        if \"hazard value\" in args:\n",
    "            self.hazard_reward = args['hazard value']\n",
    "        else:\n",
    "            self.hazard_reward = max(self.width,self.height)        \n",
    "        \n",
    "        ### initialize state index, Q matrix, and action choices ###\n",
    "        # index states for Q matrix\n",
    "        self.states = []\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                block = [i,j]\n",
    "                self.states.append(str(i) + str(j))\n",
    "        \n",
    "        # initialize action choices\n",
    "        self.action_choices = [[-1,0],[1,0],[0,-1],[0,1]]\n",
    "        \n",
    "        # initialize Q^* matrix\n",
    "        self.Q_star = np.zeros((self.width*self.height,len(self.action_choices)))\n",
    "\n",
    "        ### create custom colormap for gridworld plotting ###\n",
    "        vmax = 3.0\n",
    "        self.my_cmap = LinearSegmentedColormap.from_list('mycmap', [(0 / vmax, [0.9,0.9,0.9]),\n",
    "                                                        (1 / vmax, [1,0.5,0]),\n",
    "                                                        (2 / vmax, 'lime'),\n",
    "                                                        (3 / vmax, 'blue')]\n",
    "                                                        )\n",
    "        \n",
    "    ### world coloring function ###\n",
    "    def color_gridworld(self,ax):\n",
    "        # copy grid for plotting, add agent and goal location\n",
    "        p_grid = copy.deepcopy(self.grid)\n",
    "        p_grid[self.goal[0]][self.goal[1]] = 2   \n",
    "        p_grid[self.agent[0]][self.agent[1]] = 3   \n",
    "        \n",
    "        # plot gridworld\n",
    "        ax.pcolormesh(p_grid,edgecolors = 'k',linewidth = 0.01,cmap = self.my_cmap)\n",
    "\n",
    "        # clean up plot\n",
    "        ax.axis('off')\n",
    "        ax.set_xlim(-0.1,self.width + 1.1);\n",
    "        ax.set_ylim(-0.1,self.height + 1.1);        \n",
    "        \n",
    "    ## Q-learning function\n",
    "    def qlearn(self,**args):\n",
    "        ### set basic parameters controlling training regiment ###\n",
    "        # default parameters for the qlearning \n",
    "        gamma = 0.8                            # short term / long term learning tradeoff param\n",
    "        num_episodes = 3                     # maximum number of episodes of simulation\n",
    "        max_steps = 5*self.width*self.height  # maximum number of steps per episode\n",
    "\n",
    "        # change these default parameters if user requests\n",
    "        if \"gamma\" in args:\n",
    "            gamma = args['gamma']\n",
    "        if 'training episodes' in args:\n",
    "            num_episodes = args['training episodes']\n",
    "        if 'max steps per episode' in args:\n",
    "            max_steps = args['max steps per episode']    \n",
    "            \n",
    "        ### create starting schedule - where agent starts - at beginning of each episode ###\n",
    "        start_schedule = []  # container for holding starting positions\n",
    "        \n",
    "        # create schedule of random starting positions for each episode\n",
    "        if 'start schedule' not in args or ('start schedule' in args and args['start schedule'] == 'random'):\n",
    "            for i in range(num_episodes):\n",
    "                loc = [np.random.randint(self.height),np.random.randint(self.width)]\n",
    "                start_schedule.append(loc)\n",
    "                \n",
    "        # create exhaustive starting schedule - cycle through states sequentially\n",
    "        if 'start schedule' in args and args['start schedule'] == 'exhaustive':\n",
    "            i = 0\n",
    "            while i < num_episodes:\n",
    "                for j in range(self.width):\n",
    "                    for k in range(self.height):\n",
    "                        loc = [j,k]\n",
    "                        start_schedule.append(loc)\n",
    "                        i+=1\n",
    "\n",
    "        ### start main Q-learning loop ###\n",
    "        self.training_episodes_history = {}\n",
    "        for n in range(num_episodes):    \n",
    "            # pick this episode's starting position\n",
    "            loc = start_schedule[n]\n",
    "            \n",
    "            # update Q matrix while loc != goal\n",
    "            episode_history = []      # container for storing this episode's journey\n",
    "            episode_history.append(loc)\n",
    "            for step in range(max_steps):    \n",
    "                ### if you reach the goal end current episode immediately\n",
    "                if loc == self.goal:\n",
    "                    break\n",
    "                    \n",
    "                ### choose next action - left = 0, right = 1, up = 2, down = 3 --> if this leads you outside the gridworld you don't move\n",
    "                k = np.random.randint(len(self.action_choices))  \n",
    "                loc2 = [sum(x) for x in zip(loc, self.action_choices[k])] \n",
    "                ind_old = self.states.index(str(loc[0]) + str(loc[1]))\n",
    "\n",
    "                # if new state is outside of boundaries of grid world do not move\n",
    "                if loc2[0] > self.height-1 or loc2[0] < 0 or loc2[1] > self.width-1 or loc2[1] < 0:  \n",
    "                    loc2 = loc\n",
    "                \n",
    "                ### recieve reward     \n",
    "                # if new state is goal set reward of 0\n",
    "                if loc2 == self.goal:\n",
    "                    r_k = self.goal_reward\n",
    "                elif loc2 in self.hazards:\n",
    "                    r_k = self.hazard_reward\n",
    "                else:  # standard non-hazard square\n",
    "                    r_k = self.standard_reward\n",
    "                \n",
    "                ### Update Q function\n",
    "                ind_new = self.states.index(str(loc2[0]) + str(loc2[1]))\n",
    "                self.Q_star[ind_old,k] = r_k + gamma*max(self.Q_star[ind_new,:])\n",
    "                    \n",
    "                ### update current location of agent to one we just moved too (or stay still if grid world boundary met)\n",
    "                self.agent = loc2\n",
    "                loc = loc2\n",
    "                \n",
    "                ### update episode history container\n",
    "                episode_history.append(loc)\n",
    "                \n",
    "            ### store this episode's history\n",
    "            self.training_episodes_history[str(n)] = episode_history\n",
    "            \n",
    "        print 'q-learning process complete'\n",
    "            \n",
    "    ### animate training episode ###\n",
    "    def animate_training_episode(self,episodes):\n",
    "        # initialize figure\n",
    "        fig = plt.figure(figsize = (10,3))\n",
    "        axs = []\n",
    "        for i in range(len(episodes)):\n",
    "            ax = fig.add_subplot(1,len(episodes),i+1)\n",
    "            axs.append(ax)\n",
    "            \n",
    "        if len(episodes) == 1:\n",
    "            axs = np.array(axs)\n",
    "\n",
    "        # make a copy of the original gridworld - set at initialization\n",
    "        gridworld_orig = self.grid.copy()\n",
    "        \n",
    "        # compute maximum length of episodes animated\n",
    "        max_len = 0\n",
    "        for key in episodes:\n",
    "            l = len(self.training_episodes_history[str(key)])\n",
    "            if l > max_len:\n",
    "                max_len = l\n",
    "\n",
    "        # loop over the episode histories and plot the results\n",
    "        def show_episode(step):\n",
    "            # loop over subplots and plot current step of each episode history\n",
    "            artist = fig\n",
    "\n",
    "            for k in range(len(axs)):\n",
    "                ax = axs[k]\n",
    "                \n",
    "                # take correct episode\n",
    "                key_num = episodes[k]\n",
    "                episode_num = episodes[key_num]\n",
    "                current_episode = self.training_episodes_history[str(episode_num)]\n",
    "                                \n",
    "                # define new location of agent\n",
    "                loc = current_episode[min(step,len(current_episode)-1)]\n",
    "                self.agent = loc\n",
    "                \n",
    "                # color gridworld for this episode and step\n",
    "                self.color_gridworld(ax)\n",
    "                ax.set_title('episode = ' + str(episode_num))                \n",
    "            return artist,\n",
    "           \n",
    "        # create animation object\n",
    "        anim = animation.FuncAnimation(fig, show_episode,frames=min(100,max_len), interval=min(100,max_len), blit=True)\n",
    "        \n",
    "        # set frames per second in animation\n",
    "        IPython_display.anim_to_html(anim,fps = min(100,max_len)/float(5))\n",
    "    \n",
    "        return(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning process complete\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAD0CAYAAACrSILCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAABFVJREFUeJzt3DFu4zAARUFpsafwaXPaXENbBEbSqEiofaKZmZ4GFTsP\nUPP34zg2gP/tz90XAH4HsQESYgMkxAZIiA2QEBsgITZAQmyAhNgACbEBEmIDJMQGSIgNkBAbICE2\nQEJsgITYAAmxARJiAyTEBkiIDZAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZA4u/dF3ja\n9/24+w6wmuM49rvv8DRNbLZt2463sfP729hnjJ6f4Q6ewR2+np+J1yggITZAQmyAhNgACbEBEmID\nJMQGSIgNkBAbICE2QEJsgITYAAmxARJiAyTEBkjsxzHHZpXxLLie8axTo73Zt/f39x+ffjweQ+ev\n+IzH4zE8mDTDM6zwPcxwh6F/iWky88FrFJAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZA\nQmyAhNgACbEBEvZsYGH2bE6M7Lhs28eWy+gWzBV3uHvPZoa/451/g+cdVvg7jp6fidcoICE2QEJs\ngITYAAmxARJiAyTEBkiIDZAQGyAhNkBCbICE2AAJsQESYgMk7NnAwuzZnJhhS2bk/BWfccWezRXP\ncPeezQzfwwx3sGcD8E1iAyTEBkiIDZAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZAQmyA\nhPEsWJjxrBMzjGddcYfRwaMVxrPuPD/THYxnffIaBSTEBkiIDZAQGyAhNkBCbICE2AAJsQESYgMk\nxAZIiA2QEBsgITZAQmyAhD0bWJg9mxMz7IdcsQVz5x2ueoY7N3Vm+R5m+D3aswH4JrEBEmIDJMQG\nSIgNkBAbICE2QEJsgITYAAmxARJiAyTEBkiIDZAQGyBhzwYWZs/mxKvvhzzvMLpBcuf5q+5gz+b+\n36M9G+BXEhsgITZAQmyAhNgACbEBEmIDJMQGSIgNkBAbICE2QEJsgITYAAmxARJiAySMZ8HCjGed\nePWxoqvusMLw1Arfw6vfwXgW8CuJDZAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZAQmyA\nhNgACXs2sDB7NidefT/keYfRDZI7z89wh6ueYYbfgj2bT16jgITYAAmxARJiAyTEBkiIDZAQGyAh\nNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgYTwLFmY868QMY0Uj56/4jLvPz3CHFZ7hqjsYzwL4JrEB\nEmIDJMQGSIgNkBAbICE2QEJsgITYAAmxARJiAyTEBkiIDZAQGyBhzwYWZs/mxBV7NqP7H69+B8/g\nDl/Pz8RrFJAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZAQmyAhNgACbEBEvZsYGEz7dlM\nExtgbV6jgITYAAmxARJiAyTEBkiIDZAQGyAhNkBCbICE2AAJsQESYgMkxAZIiA2QEBsgITZAQmyA\nhNgACbEBEmIDJMQGSIgNkBAbICE2QEJsgITYAAmxARJiAyTEBkiIDZD4B4DvsNxm73xiAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067e0050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "test = my_gridworld_2(world = 'small maze')\n",
    "fig1 = plt.figure(figsize = (3,3))\n",
    "ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "test.color_gridworld(ax1)\n",
    "test.qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.animate_training_episode([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
